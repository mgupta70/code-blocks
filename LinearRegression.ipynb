{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sn6tosqJK9Qe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "840BlxHp5f4G"
   },
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "    <img src=\"imgs/SimpleLR.jpg\" alt=\"Simple Linear Regression 1\" style=\"width:400px; margin-right: 10px;\"/>\n",
    "    <img src=\"imgs/SimpleLR2.jpg\" alt=\"Simple Linear Regression 2\" style=\"width:400px;\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "isqzR4Y8LCk3"
   },
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self):\n",
    "        # set our model params to None because we will update it below in 'fit' function\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        mean_x = np.mean(X)\n",
    "        mean_y = np.mean(y)\n",
    "        sigma_x = np.sum(X)\n",
    "        sigma_xy = np.sum(X*y)\n",
    "        sigma_x2 = np.sum(X*X)\n",
    "\n",
    "        numerator = sigma_xy - mean_y*sigma_x\n",
    "        denominator = sigma_x2 - mean_x*sigma_x\n",
    "\n",
    "        self.slope = numerator/denominator\n",
    "        self.intercept = mean_y - self.slope*(mean_x)\n",
    "        # we don't return anything but updated the params which we initialised above in __init__\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        y_hat = self.slope*x_test + self.intercept\n",
    "        return y_hat\n",
    "\n",
    "    def pred_error(self, y_pred, y_test):\n",
    "        mse = np.mean((y_test-y_pred)**2)\n",
    "        return mse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "LdBg9wNcIfGe",
    "outputId": "a45beeca-0508-4ccd-cac7-224f548ad07d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (49,)\n",
      "y.shape:  (49,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,50)\n",
    "n = x.shape[0]\n",
    "m = 4\n",
    "b = 3\n",
    "noise = np.random.randn(n)*4\n",
    "y = m*x+b + noise\n",
    "print('X.shape: ',x.shape)\n",
    "print('y.shape: ', y.shape)\n",
    "\n",
    "# Divide dataset into train and val\n",
    "val_percent = 0.2\n",
    "all_indices = list(range(n))\n",
    "\n",
    "val_indices = random.sample(all_indices, int(n*val_percent))\n",
    "train_indices = list(set(all_indices).difference(set(val_indices)))\n",
    "\n",
    "X_train, y_train = x[train_indices], y[train_indices]\n",
    "X_val, y_val = x[val_indices], y[val_indices]\n",
    "\n",
    "plt.figure(figsize = (5,3))\n",
    "plt.scatter(X_train,y_train, color ='blue', label = 'train')\n",
    "plt.scatter(X_val,y_val, color ='red', label = 'val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "o_2Fg1siIjIv",
    "outputId": "cd7d05d8-879a-4a94-a016-31cda6f36d87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:  4.022535779848848\n",
      "Intercept:  2.400450122116908\n",
      "Test MSE:  19.76808341957062\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initalize model\n",
    "model = SimpleLinearRegression()\n",
    "# fit the model\n",
    "model.fit(X_train,y_train)\n",
    "# see the params found\n",
    "print('Slope: ', model.slope)\n",
    "print('Intercept: ', model.intercept)\n",
    "# get_preds\n",
    "y_preds = model.predict(X_val)\n",
    "# check error\n",
    "test_error = model.pred_error(y_preds, y_val)\n",
    "print('Test MSE: ', test_error)\n",
    "\n",
    "# print line of best fit\n",
    "xmin = np.min(x); ymin = model.slope * xmin + model.intercept\n",
    "xmax = np.max(x); ymax = model.slope * xmax + model.intercept\n",
    "\n",
    "plt.figure(figsize = (5,3))\n",
    "plt.plot([xmin, xmax], [ymin, ymax], color='black')\n",
    "plt.scatter(X_train,y_train, color ='blue', label = 'train')\n",
    "plt.scatter(X_val,y_val, color ='red', label = 'val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsFVb_9p5lsy"
   },
   "source": [
    "### Multiple Linear Regression in Closed form\n",
    "\n",
    "- theta = (X^T X)-1 XTy\n",
    "- this has x0 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/LR-closed.jpg\" alt=\"Linear Regression with GD\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yS5Kx9yYL6X8"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionClosedForm:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    def fit(self,X,y):\n",
    "        self.theta = np.linalg.pinv(X.T @ X)@X.T@y\n",
    "        # use pinv instead of inv is X.T X is non-invertible\n",
    "        # pinv making a HUGE differene in output\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        y_hat = x_test@self.theta\n",
    "        return y_hat\n",
    "\n",
    "    def pred_error(self, y_pred, y_test):\n",
    "        mse = np.mean((y_test-y_pred)**2)\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWBO0G11MAvw",
    "outputId": "aa0f1402-f644-47b2-c04f-98f2632b4a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1.shape (100,)\n",
      "X2.shape (100,)\n",
      "x0.shape (100,)\n",
      "X.shape (100, 3)\n",
      "y.shape (100,)\n",
      "X_train.shape: (80, 3), y_train.shape: (80,) \n",
      "X_val.shape: (20, 3), y_val.shape: (20,) \n"
     ]
    }
   ],
   "source": [
    "X1 = np.linspace(start=1,stop=41, num=100); print('X1.shape', X1.shape)\n",
    "X2 = np.linspace(start=5,stop=45, num=100); print('X2.shape', X2.shape)\n",
    "# X1 and X2 needs to be of same size\n",
    "\n",
    "n = X1.shape[0]\n",
    "m1 = 3\n",
    "m2 = 4.5\n",
    "b1 = 4\n",
    "b2 = 6\n",
    "noise = np.random.randn()*0\n",
    "\n",
    "x0 = np.ones_like(X1); print('x0.shape', x0.shape)\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X = np.stack([x0, X1, X2], axis=1); print('X.shape', X.shape)\n",
    "y = m1*X1 + b1 + m2*X2 + b2 + noise; print('y.shape', y.shape)\n",
    "\n",
    "# # Simple Linear Regression\n",
    "# X = np.stack([x0, X1], axis=1); print('X.shape', X.shape)\n",
    "# y = m1*X1 + b1 + noise; print('y.shape', y.shape)\n",
    "\n",
    "# Divide dataset into train and val\n",
    "val_percent = 0.2\n",
    "all_indices = list(range(n))\n",
    "\n",
    "val_indices = random.sample(all_indices, int(n*val_percent))\n",
    "train_indices = list(set(all_indices).difference(set(val_indices)))\n",
    "\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape} ')\n",
    "print(f'X_val.shape: {X_val.shape}, y_val.shape: {y_val.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvrIALGEO6Mk",
    "outputId": "05d21d0b-6e00-450d-b0eb-dbaa8a345ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [1.44444444 0.86111111 6.63888889]\n",
      "theta.shape:  (3,)\n",
      "GT: Pred ->  65.80303030303031 -> 65.80303030302905\n",
      "GT: Pred ->  235.5 -> 235.4999999999974\n",
      "GT: Pred ->  305.1969696969697 -> 305.19696969696656\n",
      "GT: Pred ->  223.37878787878788 -> 223.37878787878537\n",
      "GT: Pred ->  162.77272727272725 -> 162.77272727272523\n",
      "GT: Pred ->  290.0454545454545 -> 290.0454545454515\n",
      "GT: Pred ->  114.2878787878788 -> 114.28787878787715\n",
      "GT: Pred ->  274.8939393939394 -> 274.8939393939365\n",
      "GT: Pred ->  277.92424242424244 -> 277.92424242423954\n",
      "GT: Pred ->  159.74242424242422 -> 159.74242424242223\n",
      "GT: Pred ->  44.59090909090909 -> 44.590909090907985\n",
      "GT: Pred ->  141.56060606060606 -> 141.56060606060421\n",
      "GT: Pred ->  180.95454545454547 -> 180.95454545454328\n",
      "GT: Pred ->  77.92424242424244 -> 77.92424242424107\n",
      "GT: Pred ->  123.37878787878788 -> 123.37878787878616\n",
      "GT: Pred ->  244.59090909090907 -> 244.5909090909064\n",
      "GT: Pred ->  262.77272727272725 -> 262.7727272727244\n",
      "GT: Pred ->  247.62121212121212 -> 247.62121212120945\n",
      "GT: Pred ->  35.5 -> 35.49999999999897\n",
      "GT: Pred ->  62.77272727272728 -> 62.77272727272603\n",
      "Test MSE:  5.00894867943644e-24\n"
     ]
    }
   ],
   "source": [
    "# Initalize model\n",
    "model = LinearRegressionClosedForm()\n",
    "# fit the model\n",
    "model.fit(X_train,y_train)\n",
    "# see the params found\n",
    "print('theta: ', model.theta)\n",
    "print('theta.shape: ', model.theta.shape)\n",
    "# get_preds\n",
    "y_preds = model.predict(X_val)\n",
    "for gt,pred in zip(y_val, y_preds):\n",
    "    print('GT: Pred -> ', gt,'->', pred)\n",
    "\n",
    "# check error\n",
    "test_error = model.pred_error(y_preds, y_val)\n",
    "print('Test MSE: ', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression with Gradient Descent over expression derived from GLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/LR-GD.jpg\" alt=\"Linear Regression with GD\" style=\"width:500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "PBOHaP8VbkTY"
   },
   "outputs": [],
   "source": [
    "class LinearRegressionGLM:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def fit(self, X, y, lr=0.0001, epochs=100):\n",
    "        # init params\n",
    "        self.theta = np.random.randn(X.shape[1]) # shape: (X.shape[1],)\n",
    "        n = y.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = X@self.theta\n",
    "            self.theta = self.theta - (lr/n)*(X.T@(y_hat-y)) # Chance of error: lr -> lr/n for averaging across training data\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        y_hat = x_test@self.theta\n",
    "        return y_hat\n",
    "\n",
    "    def pred_error(self, y_pred, y_test):\n",
    "        mse = np.mean((y_test-y_pred)**2)\n",
    "        return mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CBTTw9nPWZO",
    "outputId": "ab072199-ed3d-4e53-c5b4-4afd16e68c04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:  [1.77732462 1.36135826 6.19231142]\n",
      "theta.shape:  (3,)\n",
      "GT: Pred ->  65.80303030303031 -> 64.62011751728676\n",
      "GT: Pred ->  235.5 -> 235.5314315872057\n",
      "GT: Pred ->  305.1969696969697 -> 305.7271498659224\n",
      "GT: Pred ->  223.37878787878788 -> 223.3234805822115\n",
      "GT: Pred ->  162.77272727272725 -> 162.28372555724042\n",
      "GT: Pred ->  290.0454545454545 -> 290.4672111096796\n",
      "GT: Pred ->  114.2878787878788 -> 113.45192153726359\n",
      "GT: Pred ->  274.8939393939394 -> 275.20727235343685\n",
      "GT: Pred ->  277.92424242424244 -> 278.25926010468544\n",
      "GT: Pred ->  159.74242424242422 -> 159.23173780599186\n",
      "GT: Pred ->  44.59090909090909 -> 43.256203258546876\n",
      "GT: Pred ->  141.56060606060606 -> 140.91981129850058\n",
      "GT: Pred ->  180.95454545454547 -> 180.59565206473175\n",
      "GT: Pred ->  77.92424242424244 -> 76.82806852228096\n",
      "GT: Pred ->  123.37878787878788 -> 122.60788479100924\n",
      "GT: Pred ->  244.59090909090907 -> 244.68739484095133\n",
      "GT: Pred ->  262.77272727272725 -> 262.9993213484426\n",
      "GT: Pred ->  247.62121212121212 -> 247.73938259219992\n",
      "GT: Pred ->  35.5 -> 34.10024000480122\n",
      "GT: Pred ->  62.77272727272728 -> 61.5681297660382\n",
      "Test MSE:  0.5436607009064114\n"
     ]
    }
   ],
   "source": [
    "# Initalize model\n",
    "model = LinearRegressionGLM()\n",
    "# fit the model\n",
    "model.fit(X_train,y_train, lr=0.001, epochs=1000)\n",
    "# see the params found\n",
    "print('theta: ', model.theta)\n",
    "print('theta.shape: ', model.theta.shape)\n",
    "# get_preds\n",
    "y_preds = model.predict(X_val)\n",
    "for gt,pred in zip(y_val, y_preds):\n",
    "    print('GT: Pred -> ', gt,'->', pred)\n",
    "\n",
    "# check error\n",
    "test_error = model.pred_error(y_preds, y_val)\n",
    "print('Test MSE: ', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression with Batch Gradient Descent over any Loss\n",
    "\n",
    "__theta = theta - lr*dLoss_dtheta__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionBGD:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y, lr = 0.01, epochs = 1000):\n",
    "        n, d = X.shape\n",
    "        self.theta = np.random.randn(d)\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = X @ self.theta\n",
    "            loss = self.mse(y_hat,y)\n",
    "            dL_dtheta = self.mse_derivative(X,y,y_hat)\n",
    "            self.theta = self.theta - lr * dL_dtheta\n",
    "            if epoch%50 == 0:\n",
    "                print(f\"epoch: {epoch}, train_loss: {loss}\")\n",
    "        return self.theta\n",
    "\n",
    "    def mse(self, y_hat, y):\n",
    "        return np.mean((y_hat - y)**2)/2  # for 1/2n * (y-y_hat)**2\n",
    "        \n",
    "    def mse_derivative(self, X, y, y_hat):\n",
    "        dL_dtheta = (X.T @ (y_hat-y))/ len(y)\n",
    "        return dL_dtheta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 40741.28477863934\n",
      "epoch: 50, train_loss: 3.269179066478896\n",
      "epoch: 100, train_loss: 2.7427746914371385\n",
      "epoch: 150, train_loss: 2.301132135931165\n",
      "epoch: 200, train_loss: 1.930603021657825\n",
      "epoch: 250, train_loss: 1.6197366370384878\n",
      "epoch: 300, train_loss: 1.3589260681421083\n",
      "epoch: 350, train_loss: 1.1401113097328097\n",
      "epoch: 400, train_loss: 0.9565301814820499\n",
      "epoch: 450, train_loss: 0.802509351740851\n",
      "\n",
      "268.83 ---> 269.14\n",
      "174.89 ---> 174.4\n",
      "141.56 ---> 140.79\n",
      "244.59 ---> 244.69\n",
      "38.53 ---> 36.89\n",
      "114.29 ---> 113.29\n",
      "90.05 ---> 88.84\n",
      "283.98 ---> 284.41\n",
      "80.95 ---> 79.67\n",
      "77.92 ---> 76.62\n",
      "165.8 ---> 165.24\n",
      "332.47 ---> 333.31\n",
      "168.83 ---> 168.29\n",
      "241.56 ---> 241.63\n",
      "177.92 ---> 177.46\n",
      "71.86 ---> 70.51\n",
      "135.5 ---> 134.68\n",
      "317.32 ---> 318.03\n",
      "87.02 ---> 85.79\n",
      "126.41 ---> 125.51\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionBGD()\n",
    "theta = model.fit(X_train, y_train, lr = 0.001, epochs = 500)\n",
    "y_preds = model.predict(X_val)\n",
    "print()\n",
    "for gt, pred in zip(y_val, y_preds):\n",
    "    print(f\"{round(gt,2)} ---> {round(pred,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is Batch Gradient Descent Implementation which can be slow for large sized arrays\n",
    "### Multiple Linear Regression with Mini-batch GD\n",
    "\n",
    "__theta = theta - lr*dLoss_dtheta__ for each mini-batches of training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMGD:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y, lr = 0.01, epochs = 1000, bs = 8):\n",
    "        n, d = X.shape\n",
    "        self.theta = np.random.randn(d)\n",
    "        mini_batches = n//bs if n%bs == 0 else n//bs + 1\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(mini_batches):\n",
    "                batch_end = min((i+1)*bs,n)\n",
    "                xx = X[i*bs:batch_end,:]\n",
    "                yy = y[i*bs:batch_end]\n",
    "                yy_hat = xx @ self.theta\n",
    "                loss = self.mse(yy_hat,yy)\n",
    "                epoch_loss += loss.item()\n",
    "                dL_dtheta = self.mse_derivative(xx,yy,yy_hat)\n",
    "                self.theta = self.theta - lr * dL_dtheta\n",
    "            if epoch%50 == 0:\n",
    "                print(f\"epoch: {epoch}, train_loss: {loss}\")\n",
    "        return self.theta\n",
    "\n",
    "    def mse(self, y_hat, y):\n",
    "        return np.mean((y_hat - y)**2)/2\n",
    "        \n",
    "    def mse_derivative(self, X, y, y_hat):\n",
    "        dL_dtheta = (X.T @ (y_hat-y))/len(y)\n",
    "        return dL_dtheta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 3016.893085713262\n",
      "epoch: 50, train_loss: 7.482524395578595\n",
      "epoch: 100, train_loss: 6.258141599005826\n",
      "epoch: 150, train_loss: 5.2341073951391035\n",
      "epoch: 200, train_loss: 4.377638279741499\n",
      "epoch: 250, train_loss: 3.6613151893013924\n",
      "epoch: 300, train_loss: 3.062205705173291\n",
      "epoch: 350, train_loss: 2.561129893486506\n",
      "epoch: 400, train_loss: 2.1420462773708255\n",
      "epoch: 450, train_loss: 1.7915382839688372\n",
      "\n",
      "268.83 ---> 268.86\n",
      "174.89 ---> 172.91\n",
      "141.56 ---> 138.87\n",
      "244.59 ---> 244.1\n",
      "38.53 ---> 33.64\n",
      "114.29 ---> 111.01\n",
      "90.05 ---> 86.25\n",
      "283.98 ---> 284.33\n",
      "80.95 ---> 76.97\n",
      "77.92 ---> 73.87\n",
      "165.8 ---> 163.63\n",
      "332.47 ---> 333.85\n",
      "168.83 ---> 166.72\n",
      "241.56 ---> 241.0\n",
      "177.92 ---> 176.01\n",
      "71.86 ---> 67.68\n",
      "135.5 ---> 132.68\n",
      "317.32 ---> 318.37\n",
      "87.02 ---> 83.16\n",
      "126.41 ---> 123.39\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionMGD()\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=500, bs = 8)\n",
    "y_preds = model.predict(X_val)\n",
    "print()\n",
    "for gt, pred in zip(y_val, y_preds):\n",
    "    print(f\"{round(gt,2)} ---> {round(pred,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is Batch Gradient Descent Implementation which can be slow for large sized arrays\n",
    "### Multiple Linear Regression with Stochastic GD \n",
    "- SGD means set `bs` = 1 in above\n",
    "\n",
    "__theta = theta - lr*dLoss_dtheta__ for each mini-batches of training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.0713423231458439\n",
      "epoch: 50, train_loss: 0.025154508379820605\n",
      "epoch: 100, train_loss: 0.008803050573380652\n",
      "epoch: 150, train_loss: 0.0030807081668021057\n",
      "epoch: 200, train_loss: 0.0010781220362053924\n",
      "epoch: 250, train_loss: 0.0003772986800497637\n",
      "epoch: 300, train_loss: 0.0001320391283968946\n",
      "epoch: 350, train_loss: 4.620830220211961e-05\n",
      "epoch: 400, train_loss: 1.6171018532566254e-05\n",
      "epoch: 450, train_loss: 5.659196030311171e-06\n",
      "\n",
      "268.83 ---> 268.82\n",
      "174.89 ---> 174.87\n",
      "141.56 ---> 141.53\n",
      "244.59 ---> 244.58\n",
      "38.53 ---> 38.49\n",
      "114.29 ---> 114.25\n",
      "90.05 ---> 90.01\n",
      "283.98 ---> 283.98\n",
      "80.95 ---> 80.92\n",
      "77.92 ---> 77.89\n",
      "165.8 ---> 165.78\n",
      "332.47 ---> 332.47\n",
      "168.83 ---> 168.81\n",
      "241.56 ---> 241.55\n",
      "177.92 ---> 177.9\n",
      "71.86 ---> 71.82\n",
      "135.5 ---> 135.47\n",
      "317.32 ---> 317.32\n",
      "87.02 ---> 86.98\n",
      "126.41 ---> 126.38\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionMGD()\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=500, bs = 1)\n",
    "y_preds = model.predict(X_val)\n",
    "print()\n",
    "for gt, pred in zip(y_val, y_preds):\n",
    "    print(f\"{round(gt,2)} ---> {round(pred,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__\n",
    "1. When moved from Batch GD -> Mini-batch GD -> SGD, lr requirement reduces\n",
    "2. SGD converged lot lot faster than MGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's apply regularization \n",
    "- L1\n",
    "- L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMGD_L1:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        \n",
    "    def fit(self, X, y, lr = 0.01, epochs = 1000, bs = 8, regularization = 'L1', lamda1 = 0.1):\n",
    "        self.regularization = regularization\n",
    "        self.lamda1 = lamda1\n",
    "        n, d = X.shape\n",
    "        self.theta = np.random.randn(d)\n",
    "        mini_batches = n//bs if n%bs == 0 else n//bs + 1\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(mini_batches):\n",
    "                batch_end = min((i+1)*bs,n)\n",
    "                xx = X[i*bs:batch_end,:]\n",
    "                yy = y[i*bs:batch_end]\n",
    "                yy_hat = xx @ self.theta\n",
    "                loss = self.mse(yy_hat,yy)\n",
    "                epoch_loss += loss.item()\n",
    "                dL_dtheta = self.mse_derivative(xx,yy,yy_hat)\n",
    "                self.theta = self.theta - lr * dL_dtheta\n",
    "            if epoch%50 == 0:\n",
    "                print(f\"epoch: {epoch}, train_loss: {loss}\")\n",
    "        return self.theta\n",
    "\n",
    "    def mse(self, y_hat, y):\n",
    "        if self.regularization == 'L1':\n",
    "            return np.mean((y_hat - y)**2)/2 + self.lamda1*np.sum(np.abs(self.theta))\n",
    "        else:\n",
    "            return np.mean((y_hat - y)**2)/2\n",
    "        \n",
    "    def mse_derivative(self, X, y, y_hat):\n",
    "        dL_dtheta = (X.T @ (y_hat-y))/len(y)\n",
    "        if self.regularization == 'L1':\n",
    "            dL_dtheta += self.lamda1*np.sign(self.theta)\n",
    "        return dL_dtheta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 2315.5461428943413\n",
      "epoch: 50, train_loss: 3.57482836339525\n",
      "epoch: 100, train_loss: 3.139808085945719\n",
      "epoch: 150, train_loss: 2.7761678993182244\n",
      "epoch: 200, train_loss: 2.4722046852078874\n",
      "epoch: 250, train_loss: 2.2181323018953356\n",
      "epoch: 300, train_loss: 2.005767736706614\n",
      "epoch: 350, train_loss: 1.8282686284500738\n",
      "epoch: 400, train_loss: 1.6799137528139207\n",
      "epoch: 450, train_loss: 1.5559194394711966\n",
      "\n",
      "268.83 ---> 268.84\n",
      "174.89 ---> 173.69\n",
      "141.56 ---> 139.93\n",
      "244.59 ---> 244.29\n",
      "38.53 ---> 35.57\n",
      "114.29 ---> 112.3\n",
      "90.05 ---> 87.75\n",
      "283.98 ---> 284.19\n",
      "80.95 ---> 78.54\n",
      "77.92 ---> 75.47\n",
      "165.8 ---> 164.48\n",
      "332.47 ---> 333.3\n",
      "168.83 ---> 167.55\n",
      "241.56 ---> 241.22\n",
      "177.92 ---> 176.76\n",
      "71.86 ---> 69.33\n",
      "135.5 ---> 133.79\n",
      "317.32 ---> 317.95\n",
      "87.02 ---> 84.68\n",
      "126.41 ---> 124.58\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionMGD_L1()\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=500, bs=8, regularization='L1',lamda1=0.1)\n",
    "y_preds = model.predict(X_val)\n",
    "print()\n",
    "for gt, pred in zip(y_val, y_preds):\n",
    "    print(f\"{round(gt,2)} ---> {round(pred,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add __L2 regularization__ and __Elasticnet regularization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMGD_Regularized:\n",
    "    def __init__(self, init_weights, regularization = 'L1', lamda1 = 0.1, lamda2 = 0.1, alpha1 = 0.1 ):\n",
    "        self.theta = init_weights\n",
    "        self.regularization = regularization\n",
    "        self.lamda1 = lamda1\n",
    "        self.lamda2 = lamda2\n",
    "        self.alpha1 = alpha1\n",
    "        \n",
    "    def fit(self, X, y, lr = 0.01, epochs = 1000, bs = 8):\n",
    "        n, d = X.shape\n",
    "        #self.theta = np.random.randn(d)\n",
    "        mini_batches = n//bs if n%bs == 0 else n//bs + 1\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(mini_batches):\n",
    "                batch_end = min((i+1)*bs,n)\n",
    "                xx = X[i*bs:batch_end,:]\n",
    "                yy = y[i*bs:batch_end]\n",
    "                yy_hat = xx @ self.theta\n",
    "                loss = self.mse(yy_hat,yy)\n",
    "                epoch_loss += loss.item()\n",
    "                dL_dtheta = self.mse_derivative(xx,yy,yy_hat)\n",
    "                self.theta = self.theta - lr * dL_dtheta\n",
    "            if (epoch+1)%10 == 0:\n",
    "                print(f\"epoch: {epoch+1}, train_loss: {loss}\")\n",
    "        return self.theta\n",
    "\n",
    "    def mse(self, y_hat, y):\n",
    "        if self.regularization == 'L1':\n",
    "            penalty = self.lamda1*np.sum(np.abs(self.theta))\n",
    "            return np.mean((y_hat - y)**2)/2 + penalty\n",
    "        elif self.regularization == 'L2':\n",
    "            penalty = self.lamda2*np.sum(self.theta**2)\n",
    "            return np.mean((y_hat - y)**2)/2 + penalty\n",
    "        elif self.regularization == 'Elasticnet':\n",
    "            weighted_penalty = self.alpha1* self.lamda1*np.sum(np.abs(self.theta)) + (1-self.alpha1)*self.lamda2*np.sum(self.theta.T**2)\n",
    "            return np.mean((y_hat - y)**2)/2 + weighted_penalty\n",
    "        else:\n",
    "            return np.mean((y_hat - y)**2)/2\n",
    "        \n",
    "    def mse_derivative(self, X, y, y_hat):\n",
    "        dL_dtheta = (X.T @ (y_hat-y))/ len(y) \n",
    "        if self.regularization == 'L1':\n",
    "            dL_dtheta += self.lamda1 * np.sign(self.theta)\n",
    "        elif self.regularization == 'L2':\n",
    "            dL_dtheta += 2 * self.lamda2 * self.theta\n",
    "        elif self.regularization == 'Elasticnet':\n",
    "            dL_dtheta += self.alpha1 * self.lamda1 * np.sign(self.theta) + \\\n",
    "                         (1 - self.alpha1) * 2 * self.lamda2 * self.theta\n",
    "        return dL_dtheta\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's observe impact of different types of Regularization when model is initialized the same__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, train_loss: 6.939439103934874\n",
      "\n",
      "epoch: 10, train_loss: 9.190242040791583\n",
      "\n",
      "epoch: 10, train_loss: 8.064886999319636\n"
     ]
    }
   ],
   "source": [
    "init_weights = np.random.randn(X_train.shape[1])\n",
    "\n",
    "weights_dict = {}\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='L1',lamda1=0.1)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=10, bs=8)\n",
    "y_preds = model.predict(X_val)\n",
    "weights_dict['L1'] = theta\n",
    "print()\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='L2',lamda2=0.1)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=10, bs=8)\n",
    "y_preds = model.predict(X_val)\n",
    "weights_dict['L2'] = theta\n",
    "print()\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='Elasticnet',lamda1=0.1, lamda2=0.1, alpha1=0.5)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=10, bs=8)\n",
    "weights_dict['Elastic'] = theta\n",
    "y_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1': array([-0.66275327,  3.23353082,  4.58875854]),\n",
       " 'L2': array([-0.66217778,  3.2340638 ,  4.58762088]),\n",
       " 'Elastic': array([-0.66246538,  3.23379749,  4.58818951])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's observe impact of different values of `lamda1` in `L1` Regularization when model is initialized the same__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, train_loss: 5.920859890570223\n",
      "epoch: 20, train_loss: 5.736951895341615\n",
      "epoch: 30, train_loss: 5.559285351402725\n",
      "epoch: 40, train_loss: 5.387836490793091\n",
      "epoch: 50, train_loss: 5.222387324170967\n",
      "epoch: 60, train_loss: 5.0627275098298385\n",
      "epoch: 70, train_loss: 4.908654085333505\n",
      "epoch: 80, train_loss: 4.759971208547345\n",
      "epoch: 90, train_loss: 4.619024356494699\n",
      "epoch: 100, train_loss: 4.485685961174973\n",
      "\n",
      "epoch: 10, train_loss: 82.15366870116148\n",
      "epoch: 20, train_loss: 81.40960646517179\n",
      "epoch: 30, train_loss: 81.20035140403783\n",
      "epoch: 40, train_loss: 81.00049285218819\n",
      "epoch: 50, train_loss: 80.8073526747799\n",
      "epoch: 60, train_loss: 80.62257324799583\n",
      "epoch: 70, train_loss: 80.44251615780794\n",
      "epoch: 80, train_loss: 80.26587643834547\n",
      "epoch: 90, train_loss: 80.09242510249575\n",
      "epoch: 100, train_loss: 79.9268569654515\n",
      "\n",
      "epoch: 10, train_loss: 7710.594388933325\n",
      "epoch: 20, train_loss: 7619.567071858782\n",
      "epoch: 30, train_loss: 7594.187814497478\n",
      "epoch: 40, train_loss: 7606.041607105273\n",
      "epoch: 50, train_loss: 7463.9638034682885\n",
      "epoch: 60, train_loss: 7555.8110776042495\n",
      "epoch: 70, train_loss: 7510.597293649521\n",
      "epoch: 80, train_loss: 7507.178061319233\n",
      "epoch: 90, train_loss: 7518.212014851362\n",
      "epoch: 100, train_loss: 7507.627716181048\n",
      "Final results: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.1: array([0.025, 2.769, 4.982]),\n",
       " 10: array([1.000e-03, 2.706e+00, 5.030e+00]),\n",
       " 1000: array([-0.047,  0.319,  6.271])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights = np.random.randn(X_train.shape[1])\n",
    "\n",
    "weights_dict = {}\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='L1',lamda1=0.1)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=100, bs=8)\n",
    "y_preds = model.predict(X_val)\n",
    "weights_dict[0.1] = np.round(theta,3)\n",
    "print()\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='L1',lamda1=10)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=100, bs=8)\n",
    "y_preds = model.predict(X_val)\n",
    "weights_dict[10] = np.round(theta,3)\n",
    "print()\n",
    "\n",
    "model = LinearRegressionMGD_Regularized(init_weights = init_weights, regularization='L1',lamda1=1000)\n",
    "theta = model.fit(X_train, y_train, lr = 0.0001, epochs=100, bs=8)\n",
    "weights_dict[1000] = np.round(theta,3)\n",
    "y_preds = model.predict(X_val)\n",
    "\n",
    "print('Final results: \\n')\n",
    "weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkyKB6ixRnYa"
   },
   "source": [
    "\n",
    "<a href=\"https://github.com/mgupta70/literature/blob/main/Linear%20Regression.pdf\"> Click here</a> for for intriguing __Q&A__ on Linear Regression :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt5fSurQSXBX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
