{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JzCugtH4Ntnq"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b93noFCMw5aL"
   },
   "source": [
    "__We show why feature normalization is crucial for training Neural Networks with an experiemntal setup for Linear Regression in 2 variables__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dojxSq2Q4Rkw"
   },
   "source": [
    "### Simple Neural Network for Regression\n",
    "  - no activation at output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doE_RM5txUpg"
   },
   "source": [
    "<img src=\"imgs/neuron1.jpg\" alt=\"Neuron1\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3DgBtG5tZ_D"
   },
   "source": [
    "<img src=\"imgs/neuron2.jpg\" alt=\"Neuron2\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v2vvML71SInR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "  return np.mean((y_pred-y_true)**2)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "##################################################################\n",
    "#### Developing Neural Network from scratch for Regression #######\n",
    "##################################################################\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias, activation = 'sigmoid'):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    self.activation = activation\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    #self.inputs = self.inputs\n",
    "    z = np.dot(self.weights, inputs) + self.bias\n",
    "    if self.activation=='sigmoid':\n",
    "      h = sigmoid(z)\n",
    "    else:\n",
    "      h = z\n",
    "    return h\n",
    "\n",
    "\n",
    "class ThreeNeuronsNNRegression:\n",
    "\n",
    "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3, print_loss=True):\n",
    "    self.print_loss = print_loss\n",
    "    self.weights1 = weights1\n",
    "    self.weights2 = weights2\n",
    "    self.weights3 = weights3\n",
    "    self.bias1 = bias1\n",
    "    self.bias2 = bias2\n",
    "    self.bias3 = bias3\n",
    "    self.n1 = Neuron(self.weights1, self.bias1)\n",
    "    self.n2 = Neuron(self.weights2, self.bias2)\n",
    "    self.n3 = Neuron(self.weights3, self.bias3, activation='none')\n",
    "\n",
    "\n",
    "  def ForwardAndBackward(self, X,Y, epochs=10, lr=0.001):\n",
    "    for epoch in range(epochs):\n",
    "      loss=0\n",
    "      for x,y in zip(X,Y):\n",
    "\n",
    "      # Run feedforward part\n",
    "        self.h1 = self.n1.feedforward(x)\n",
    "        self.h2 = self.n2.feedforward(x)\n",
    "        inp_o = np.array([self.h1,self.h2]) # input to output neuron\n",
    "        self.h3 = self.n3.feedforward(inp_o)\n",
    "\n",
    "      # after feedforward, we calculate loss\n",
    "        loss += mse_loss(self.h3, y)\n",
    "\n",
    "      # TO update parameters - we need gradients\n",
    "      ## Let's calculate them now\n",
    "\n",
    "        self.del_h1w1 =  self.h1 *(1-self.h1)*x[0]\n",
    "        self.del_h1w2 =  self.h1 *(1-self.h1)*x[1]\n",
    "        self.del_h1b1 =  self.h1 *(1-self.h1)\n",
    "\n",
    "        self.del_h2w3 =  self.h2 *(1-self.h2)*x[0]\n",
    "        self.del_h2w4 =  self.h2 *(1-self.h2)*x[1]\n",
    "        self.del_h2b2 =  self.h2 *(1-self.h2)\n",
    "\n",
    "        self.del_h3w5 =  self.h1\n",
    "        self.del_h3w6 =  self.h2\n",
    "        self.del_h3b3 =  np.ones(1)\n",
    "\n",
    "        self.del_h3h1 =  self.weights3[0]\n",
    "        self.del_h3h2 =  self.weights3[1]\n",
    "\n",
    "        self.del_Lossh3 = 2*(self.h3-y)\n",
    "\n",
    "      # Now we add backpropagation part (Chain rule)\n",
    "\n",
    "        del_L1w1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w1\n",
    "        del_L1w2 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w2\n",
    "        del_L1b1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1b1\n",
    "\n",
    "        del_L1w3 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w3\n",
    "        del_L1w4 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w4\n",
    "        del_L1b2 = self.del_Lossh3 * self.del_h3h2 * self.del_h2b2\n",
    "\n",
    "        del_L1w5 = self.del_Lossh3 * self.del_h3w5\n",
    "        del_L1w6 = self.del_Lossh3 * self.del_h3w6\n",
    "        del_L1b3 = self.del_Lossh3 * self.del_h3b3\n",
    "\n",
    "      # update parameters\n",
    "        self.weights1[0] = self.weights1[0] - lr*(del_L1w1.item()) # w1\n",
    "        self.weights1[1] = self.weights1[1] - lr*(del_L1w2.item()) # w2\n",
    "        self.weights2[0] = self.weights2[0] - lr*(del_L1w3.item()) # w3\n",
    "        self.weights2[1] = self.weights2[1] - lr*(del_L1w4.item()) # w4\n",
    "        self.weights3[0] = self.weights3[0] - lr*(del_L1w5.item()) # w5\n",
    "        self.weights3[1] = self.weights3[1] - lr*(del_L1w6.item()) # w6\n",
    "        self.bias1[0] = self.bias1[0] - lr*(del_L1b1.item())       # b1\n",
    "        self.bias2[0] = self.bias2[0] - lr*(del_L1b2.item())       # b2\n",
    "        self.bias3[0] = self.bias3[0] - lr*(del_L1b3.item())       # b3\n",
    "\n",
    "      # print loss after each epoch\n",
    "      if self.print_loss and epoch%10==0:\n",
    "        print(f'Epoch: {epoch}: {x}, {y},  MSE train loss: {loss}')\n",
    "\n",
    "\n",
    "  def predict(self,x):\n",
    "    h1 = self.n1.feedforward(x)\n",
    "    h2 = self.n2.feedforward(x)\n",
    "    inp_o = np.array([h1,h2]) # input to output neuron\n",
    "    h3 = self.n3.feedforward(inp_o)\n",
    "    return h3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c5LT9L_u2RI"
   },
   "source": [
    "## Case-1: Both X1 and X2 have about the same range and close to 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8JbznSaPFAV",
    "outputId": "6f538ac2-1b4c-43f0-e6ef-69c1ee84ac62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (20, 2)\n",
      "Y.shape (20,)\n",
      "Epoch: 0: [1.1 2.5], 30.6580555957063,  MSE train loss: 6593.7024903588135\n",
      "Epoch: 10: [1.1 2.5], 30.6580555957063,  MSE train loss: 28.033292616935647\n",
      "Epoch: 20: [1.1 2.5], 30.6580555957063,  MSE train loss: 6.643828100887328\n",
      "Epoch: 30: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.69129849662327\n",
      "Epoch: 40: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.0714037618358416\n",
      "Epoch: 50: [1.1 2.5], 30.6580555957063,  MSE train loss: 2.979279670857386\n",
      "Epoch: 60: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.0022128492885662\n",
      "Epoch: 70: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.0319872127089145\n",
      "Epoch: 80: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.042588676023509\n",
      "Epoch: 90: [1.1 2.5], 30.6580555957063,  MSE train loss: 3.031479494544288\n",
      "\n",
      "           Actual -----> Predictions: \n",
      "16.1080555957063 ----> 17.144008767070492\n",
      "16.873845069390512 ----> 17.443150690741483\n",
      "17.63963454307472 ----> 17.800451833342382\n",
      "18.405424016758932 ----> 18.227718253878454\n",
      "19.171213490443144 ----> 18.73632026689692\n",
      "19.937002964127352 ----> 19.33571510093866\n",
      "20.702792437811564 ----> 20.03153798495254\n",
      "21.468581911495775 ----> 20.82348550225038\n",
      "22.234371385179983 ----> 21.703449497960325\n",
      "23.000160858864195 ----> 22.654541348737933\n",
      "23.765950332548403 ----> 23.651623303002765\n",
      "24.531739806232615 ----> 24.663624669668827\n",
      "25.297529279916827 ----> 25.657323234180453\n",
      "26.06331875360104 ----> 26.601685054188152\n",
      "26.829108227285246 ----> 27.47161435752762\n",
      "27.594897700969458 ----> 28.2502204686743\n",
      "28.360687174653666 ----> 28.929301575447546\n",
      "29.12647664833788 ----> 29.508338541421246\n",
      "29.89226612202209 ----> 29.99262215987914\n",
      "30.6580555957063 ----> 30.391152947890944\n"
     ]
    }
   ],
   "source": [
    "# y = m1x1 + m2x2 + c\n",
    "X1 = np.linspace(start=0,stop=1.1, num=20)#; print('X1.shape', X1.shape)\n",
    "X2 = np.linspace(start=0,stop=2.5, num=20)#; print('X2.shape', X2.shape)\n",
    "# X1 and X2 needs to be of same size\n",
    "\n",
    "n = X1.shape[0]\n",
    "m1 = 3\n",
    "m2 = 4.5\n",
    "b1 = 4\n",
    "b2 = 6\n",
    "noise = np.random.randn()*5\n",
    "\n",
    "# x0 = np.ones_like(X1); print('x0.shape', x0.shape) # not required in neural networks because Neuron has in-built bias parameter\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X = np.stack([X1, X2], axis=1); print('X.shape', X.shape)\n",
    "Y = m1*X1 + b1 + m2*X2 + b2 + noise; print('Y.shape', Y.shape)\n",
    "\n",
    "weights1 = np.random.randn(2)\n",
    "weights2 = np.random.randn(2)\n",
    "weights3 = np.random.randn(2)\n",
    "bias1 = np.random.randn(1)\n",
    "bias2 = np.random.randn(1)\n",
    "bias3 = np.random.randn(1)\n",
    "\n",
    "# Train our neural network!\n",
    "model = ThreeNeuronsNNRegression(weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "model.ForwardAndBackward(X,Y, lr=0.01, epochs=100)\n",
    "\n",
    "print('\\n           Actual -----> Predictions: ')\n",
    "for x,y in zip(X,Y):\n",
    "  print(f'{y} ----> {model.predict(x).item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8djJtNbvbm4"
   },
   "source": [
    "### We see that model converges well in Case-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_JkcGIkvVf-"
   },
   "source": [
    "## Case-2: Both X1 and X2 have about the same range and >> 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSet3-rbvTiD",
    "outputId": "98536ced-cf37-44aa-f578-6067895a0340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (20, 2)\n",
      "Y.shape (20,)\n",
      "Epoch: 0: [11.1 12.5], 102.88572777832591,  MSE train loss: 91971.89530364238\n",
      "Epoch: 10: [11.1 12.5], 102.88572777832591,  MSE train loss: 400.8711939265631\n",
      "Epoch: 20: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.6620718395914\n",
      "Epoch: 30: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.6622781985129\n",
      "Epoch: 40: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.66225747501807\n",
      "Epoch: 50: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.662236784727\n",
      "Epoch: 60: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.6622161664455\n",
      "Epoch: 70: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.6621955946159\n",
      "Epoch: 80: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.66217504382035\n",
      "Epoch: 90: [11.1 12.5], 102.88572777832591,  MSE train loss: 401.6621544886955\n",
      "\n",
      "           Actual -----> Predictions: \n",
      "88.33572777832592 ----> 96.63876022337257\n",
      "89.10151725201013 ----> 96.63876103738897\n",
      "89.86730672569435 ----> 96.63876164050272\n",
      "90.63309619937854 ----> 96.63876207468117\n",
      "91.39888567306276 ----> 96.63876237435812\n",
      "92.16467514674696 ----> 96.63876256771754\n",
      "92.93046462043117 ----> 96.6387626777649\n",
      "93.69625409411539 ----> 96.63876272322067\n",
      "94.46204356779961 ----> 96.63876271926489\n",
      "95.22783304148382 ----> 96.63876267815728\n",
      "95.99362251516803 ----> 96.63876260975293\n",
      "96.75941198885224 ----> 96.63876252193108\n",
      "97.52520146253644 ----> 96.63876242095111\n",
      "98.29099093622065 ----> 96.6387623117475\n",
      "99.05678040990486 ----> 96.63876219817453\n",
      "99.82256988358907 ----> 96.63876208320835\n",
      "100.58835935727328 ----> 96.63876196911389\n",
      "101.35414883095748 ----> 96.63876185758261\n",
      "102.1199383046417 ----> 96.63876174984577\n",
      "102.88572777832591 ----> 96.63876164676729\n"
     ]
    }
   ],
   "source": [
    "# y = m1x1 + m2x2 + c\n",
    "X1 = np.linspace(start=10,stop=11.1, num=20)#; print('X1.shape', X1.shape)\n",
    "X2 = np.linspace(start=10,stop=12.5, num=20)#; print('X2.shape', X2.shape)\n",
    "# X1 and X2 needs to be of same size\n",
    "\n",
    "n = X1.shape[0]\n",
    "m1 = 3\n",
    "m2 = 4.5\n",
    "b1 = 4\n",
    "b2 = 6\n",
    "noise = np.random.randn()*5\n",
    "\n",
    "# x0 = np.ones_like(X1); print('x0.shape', x0.shape) # not required in neural networks because Neuron has in-built bias parameter\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X = np.stack([X1, X2], axis=1); print('X.shape', X.shape)\n",
    "Y = m1*X1 + b1 + m2*X2 + b2 + noise; print('Y.shape', Y.shape)\n",
    "\n",
    "weights1 = np.random.randn(2)\n",
    "weights2 = np.random.randn(2)\n",
    "weights3 = np.random.randn(2)\n",
    "bias1 = np.random.randn(1)\n",
    "bias2 = np.random.randn(1)\n",
    "bias3 = np.random.randn(1)\n",
    "\n",
    "# Train our neural network!\n",
    "model = ThreeNeuronsNNRegression(weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "model.ForwardAndBackward(X,Y, lr=0.01, epochs=100)\n",
    "\n",
    "print('\\n           Actual -----> Predictions: ')\n",
    "for x,y in zip(X,Y):\n",
    "  print(f'{y} ----> {model.predict(x).item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZu8ZPqIvqyK"
   },
   "source": [
    "#### We see that model error now saturates in relatively higher range of 404. Also all predictions are of same values ==> Not a good convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB_FEN-2wKLc"
   },
   "source": [
    "## Case-3: X1 in range 0-1 and X2 inrange > > 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvjKR7zsvTk3",
    "outputId": "9d2bd124-0e06-4e15-e7cd-63860b8e1873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (20, 2)\n",
      "Y.shape (20,)\n",
      "Epoch: 0: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 55113.28697711192\n",
      "Epoch: 10: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.0067815050575\n",
      "Epoch: 20: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6619647360742\n",
      "Epoch: 30: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.66215269394803\n",
      "Epoch: 40: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.66215274744155\n",
      "Epoch: 50: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6621527474567\n",
      "Epoch: 60: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6621527474567\n",
      "Epoch: 70: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6621527474567\n",
      "Epoch: 80: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6621527474567\n",
      "Epoch: 90: [ 1.1 12.5], 73.0336991110552,  MSE train loss: 401.6621527474567\n",
      "\n",
      "           Actual -----> Predictions: \n",
      "58.4836991110552 ----> 66.78673337688892\n",
      "59.24948858473942 ----> 66.78673337689855\n",
      "60.015278058423625 ----> 66.7867333769059\n",
      "60.78106753210783 ----> 66.78673337691151\n",
      "61.54685700579205 ----> 66.78673337691582\n",
      "62.31264647947626 ----> 66.7867333769191\n",
      "63.07843595316046 ----> 66.78673337692162\n",
      "63.84422542684467 ----> 66.78673337692354\n",
      "64.61001490052888 ----> 66.78673337692501\n",
      "65.37580437421309 ----> 66.78673337692612\n",
      "66.1415938478973 ----> 66.78673337692699\n",
      "66.90738332158152 ----> 66.78673337692766\n",
      "67.67317279526573 ----> 66.78673337692815\n",
      "68.43896226894994 ----> 66.78673337692854\n",
      "69.20475174263414 ----> 66.78673337692884\n",
      "69.97054121631835 ----> 66.78673337692906\n",
      "70.73633069000256 ----> 66.78673337692923\n",
      "71.50212016368677 ----> 66.78673337692936\n",
      "72.26790963737099 ----> 66.78673337692948\n",
      "73.0336991110552 ----> 66.78673337692955\n"
     ]
    }
   ],
   "source": [
    "# y = m1x1 + m2x2 + c\n",
    "X1 = np.linspace(start=0,stop=1.1, num=20)#; print('X1.shape', X1.shape)\n",
    "X2 = np.linspace(start=10,stop=12.5, num=20)#; print('X2.shape', X2.shape)\n",
    "# X1 and X2 needs to be of same size\n",
    "\n",
    "n = X1.shape[0]\n",
    "m1 = 3\n",
    "m2 = 4.5\n",
    "b1 = 4\n",
    "b2 = 6\n",
    "noise = np.random.randn()*5\n",
    "\n",
    "# x0 = np.ones_like(X1); print('x0.shape', x0.shape) # not required in neural networks because Neuron has in-built bias parameter\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X = np.stack([X1, X2], axis=1); print('X.shape', X.shape)\n",
    "Y = m1*X1 + b1 + m2*X2 + b2 + noise; print('Y.shape', Y.shape)\n",
    "\n",
    "weights1 = np.random.randn(2)\n",
    "weights2 = np.random.randn(2)\n",
    "weights3 = np.random.randn(2)\n",
    "bias1 = np.random.randn(1)\n",
    "bias2 = np.random.randn(1)\n",
    "bias3 = np.random.randn(1)\n",
    "\n",
    "# Train our neural network!\n",
    "model = ThreeNeuronsNNRegression(weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "model.ForwardAndBackward(X,Y, lr=0.01, epochs=100)\n",
    "\n",
    "print('\\n           Actual -----> Predictions: ')\n",
    "for x,y in zip(X,Y):\n",
    "  print(f'{y} ----> {model.predict(x).item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oykoxZ5ZwZc5"
   },
   "source": [
    "### Case-3 also suffers from same problem as Case-2 ==> Not a good fit ==> Normalisation is important"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
