{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTjZ_IKDHeGH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ],
      "metadata": {
        "id": "C8ky8bsDHnlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dictionary or vocabulary\n",
        "vocab = list(set([word for phrase in train_data.keys() for word in phrase.split(' ') ]))\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsBiU5hCHolV",
        "outputId": "84a22e55-6a18-44c8-b202-ac9e7b4db3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['at',\n",
              " 'sad',\n",
              " 'bad',\n",
              " 'and',\n",
              " 'now',\n",
              " 'not',\n",
              " 'happy',\n",
              " 'am',\n",
              " 'all',\n",
              " 'good',\n",
              " 'is',\n",
              " 'was',\n",
              " 'right',\n",
              " 'i',\n",
              " 'or',\n",
              " 'this',\n",
              " 'very',\n",
              " 'earlier']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_idx = {w: i for i,w in enumerate(vocab)}\n",
        "idx_to_word = {i: w for i,w in enumerate(vocab)}\n",
        "\n",
        "print(word_to_idx)\n",
        "print(idx_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79R4CxNkIBLI",
        "outputId": "b9f077be-a4cc-4ad5-bbda-cb189392a647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'at': 0, 'sad': 1, 'bad': 2, 'and': 3, 'now': 4, 'not': 5, 'happy': 6, 'am': 7, 'all': 8, 'good': 9, 'is': 10, 'was': 11, 'right': 12, 'i': 13, 'or': 14, 'this': 15, 'very': 16, 'earlier': 17}\n",
            "{0: 'at', 1: 'sad', 2: 'bad', 3: 'and', 4: 'now', 5: 'not', 6: 'happy', 7: 'am', 8: 'all', 9: 'good', 10: 'is', 11: 'was', 12: 'right', 13: 'i', 14: 'or', 15: 'this', 16: 'very', 17: 'earlier'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(y):\n",
        "  return np.exp(y)/(np.sum(np.exp(y)))\n",
        "\n",
        "## later\n",
        "def softmax_der(p, y_true):\n",
        "  ''' y_true shape: output_size,1'''\n",
        "  return np.where(y_true==1, p-1, p)"
      ],
      "metadata": {
        "id": "iTsKSJA98FS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex;\">\n",
        "    <img src=\"imgs/RNN1.jpg\" alt=\"RNN1\" style=\"width:400px; margin-right: 10px;\"/>\n",
        "    <img src=\"imgs/RNN2.jpg\" alt=\"RNN2\" style=\"width:400px;\"/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "u3qEsWTaFvm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display: flex;\">\n",
        "    <img src=\"imgs/RNN3.jpg\" alt=\"RNN3\" style=\"width:400px; margin-right: 10px;\"/>\n",
        "    <img src=\"imgs/RNN4.jpg\" alt=\"RNN4\" style=\"width:400px;\"/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "8x6jPncAFvcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "  def __init__(self, vocab_size, hidden_size, output_size):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # W&B\n",
        "    self.Wxh = np.random.randn(self.hidden_size, self.vocab_size)/1000\n",
        "    self.Whh = np.random.randn(self.hidden_size, self.hidden_size)/1000\n",
        "    self.Why = np.random.randn(self.output_size, self.hidden_size)/1000\n",
        "    self.bh = np.random.randn(self.hidden_size, 1)\n",
        "    self.by = np.random.randn(self.output_size, 1)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    # caching for backprop\n",
        "    self.last_inputs = inputs\n",
        "    self.hs = {0:h}\n",
        "\n",
        "    for i,input in enumerate(inputs):\n",
        "      h = np.tanh(np.dot(self.Wxh,input) + np.dot(self.Whh, h) + self.bh)\n",
        "      self.hs[i+1] = h\n",
        "\n",
        "    # final y\n",
        "    last_key = list(self.hs.keys())[-1]\n",
        "    self.h_last = self.hs[last_key]\n",
        "    y = np.dot(self.Why, self.h_last)+self.by\n",
        "\n",
        "    return y\n",
        "\n",
        "  def backprop(self, dL_dy, lr):\n",
        "\n",
        "    # easiest to calculate dL_dWhy and dL_dby - because in many-to-one these 2 happen only at last step\n",
        "\n",
        "    # y = Why*h_last + by\n",
        "    # dL_dWhy = dL_dy * dy_dWhy ; dy_dWhy  = h_last\n",
        "    dL_dWhy = np.dot(dL_dy, self.h_last.T)\n",
        "\n",
        "    # dL_dby = dL_dy * dy_dby ; dy_dby  = 1\n",
        "    dL_dby = dL_dy\n",
        "\n",
        "    # difficult to calculate: dL_dWxh, dL_dWhh and dL_dbh - because in  many-to-one they happen across all timesteps\n",
        "    # For these 3 - we perform BPTT\n",
        "\n",
        "    n = len(self.last_inputs)\n",
        "    dL_dWxh = np.zeros(self.Wxh.shape)\n",
        "    dL_dWhh = np.zeros(self.Whh.shape)\n",
        "    dL_dbh = np.zeros(self.bh.shape)\n",
        "\n",
        "    #dy_dh_last = self.Why  ## last\n",
        "\n",
        "    # dL_dh = dL_dy * dy_dh ; for last : dy_dh_last = self.Why\n",
        "    dL_dhn = np.dot(self.Why.T, dL_dy) #(64,1) # basically dL_dhn\n",
        "    dL_dh = dL_dhn # we will keep updating dL_dh and for last time step it is dL_dhn\n",
        "\n",
        "    for t in reversed(range(n)):\n",
        "      temp = (1-self.hs[t+1]**2)*dL_dh # (64,1)\n",
        "      dL_dbh += temp\n",
        "      dL_dWhh += temp@self.hs[t].T #(64,64)\n",
        "      dL_dWxh += temp@self.last_inputs[t].T\n",
        "\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
        "      dL_dh = self.Whh @ temp\n",
        "\n",
        "    # clip gradients to prevent exploding and vanishing gradient\n",
        "    for d in [dL_dWxh, dL_dWhh, dL_dWhy, dL_dbh, dL_dby]:\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    self.Whh -= lr * dL_dWhh\n",
        "    self.Wxh -= lr * dL_dWxh\n",
        "    self.Why -= lr * dL_dWhy\n",
        "    self.bh -= lr * dL_dbh\n",
        "    self.by -= lr * dL_dby\n",
        "\n"
      ],
      "metadata": {
        "id": "U6IiMpe3Am_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_to_x(text):\n",
        "  x = []\n",
        "  for w in text:\n",
        "    one_hot = np.zeros((vocab_size,1))\n",
        "    idx = word_to_idx[w]\n",
        "    one_hot[idx] = 1\n",
        "    x.append(one_hot)\n",
        "  return x\n",
        "\n",
        "def processData(data):\n",
        "  X = []\n",
        "  Y = []\n",
        "  Y_ = [] # one-hot\n",
        "  for phrase,sentiment in list(data.items()):\n",
        "    words = phrase.split(' ')\n",
        "    x = phrase_to_x(words)\n",
        "    X.append(x)\n",
        "    label = int(sentiment)\n",
        "    Y.append(label)\n",
        "    y_ = np.zeros((output_size,1))\n",
        "    y_[label] = 1\n",
        "    Y_.append(y_)\n",
        "  return X, Y, Y_\n"
      ],
      "metadata": {
        "id": "2PzBGqTdDHMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_idx)\n",
        "output_size = 2\n",
        "hidden_size = 64\n",
        "train_X, train_Y, train_Y_ = processData(train_data)\n",
        "test_X, test_Y, test_Y_ = processData(test_data)"
      ],
      "metadata": {
        "id": "ZgVPSunqXGvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loop(model, X,Y, backprop=True):\n",
        "  num_correct = 0\n",
        "  loss = 0\n",
        "  for x,y_true in zip(X,Y):\n",
        "    logit = model.forward(x)\n",
        "    probab = softmax(logit)\n",
        "    num_correct+=int(np.argmax(probab)==np.argmax(y_true))\n",
        "    dL_dy = np.where(y_true==1, probab-1, probab)\n",
        "    loss -= np.log(np.dot(probab.T, y_true)) # -ln(Pc) if c = true class\n",
        "    if backprop:\n",
        "      model.backprop(dL_dy=dL_dy, lr=0.02)\n",
        "\n",
        "  return loss/len(X), num_correct\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NdkcEcYCXjfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(vocab_size=len(vocab), hidden_size=64, output_size=2)\n",
        "\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = loop(model, train_X, train_Y_, True)\n",
        "  test_loss, test_acc = loop(model, test_X, test_Y_, False)\n",
        "  if epoch%50==0:\n",
        "    print(f'Epoch: {epoch}, train_loss = {train_loss}, train_acc = {train_acc}')\n",
        "    print(f'Epoch: {epoch}, test_loss = {test_loss}, test_acc = {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf63lp-hXm5C",
        "outputId": "4b8a3585-31a3-4364-e7b9-28e9e0228b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train_loss = [[0.81082529]], train_acc = 23\n",
            "Epoch: 0, test_loss = [[0.77319327]], test_acc = 10\n",
            "Epoch: 50, train_loss = [[0.74207345]], train_acc = 21\n",
            "Epoch: 50, test_loss = [[0.72460972]], test_acc = 10\n",
            "Epoch: 100, train_loss = [[0.73354868]], train_acc = 25\n",
            "Epoch: 100, test_loss = [[0.72111949]], test_acc = 10\n",
            "Epoch: 150, train_loss = [[0.72627744]], train_acc = 22\n",
            "Epoch: 150, test_loss = [[0.71934677]], test_acc = 10\n",
            "Epoch: 200, train_loss = [[0.71875362]], train_acc = 23\n",
            "Epoch: 200, test_loss = [[0.71560538]], test_acc = 9\n",
            "Epoch: 250, train_loss = [[0.71186055]], train_acc = 29\n",
            "Epoch: 250, test_loss = [[0.70686889]], test_acc = 10\n",
            "Epoch: 300, train_loss = [[0.68495242]], train_acc = 30\n",
            "Epoch: 300, test_loss = [[0.63875268]], test_acc = 11\n",
            "Epoch: 350, train_loss = [[0.31242298]], train_acc = 49\n",
            "Epoch: 350, test_loss = [[0.28931403]], test_acc = 18\n",
            "Epoch: 400, train_loss = [[0.03335168]], train_acc = 58\n",
            "Epoch: 400, test_loss = [[0.04897648]], test_acc = 20\n",
            "Epoch: 450, train_loss = [[0.82107097]], train_acc = 40\n",
            "Epoch: 450, test_loss = [[1.1346436]], test_acc = 12\n",
            "Epoch: 500, train_loss = [[0.0136018]], train_acc = 58\n",
            "Epoch: 500, test_loss = [[0.02187545]], test_acc = 20\n",
            "Epoch: 550, train_loss = [[0.00587958]], train_acc = 58\n",
            "Epoch: 550, test_loss = [[0.00914212]], test_acc = 20\n",
            "Epoch: 600, train_loss = [[0.00375602]], train_acc = 58\n",
            "Epoch: 600, test_loss = [[0.00593208]], test_acc = 20\n",
            "Epoch: 650, train_loss = [[0.00274988]], train_acc = 58\n",
            "Epoch: 650, test_loss = [[0.00439393]], test_acc = 20\n",
            "Epoch: 700, train_loss = [[0.00214784]], train_acc = 58\n",
            "Epoch: 700, test_loss = [[0.00347904]], test_acc = 20\n",
            "Epoch: 750, train_loss = [[0.00173898]], train_acc = 58\n",
            "Epoch: 750, test_loss = [[0.00283716]], test_acc = 20\n",
            "Epoch: 800, train_loss = [[0.00144174]], train_acc = 58\n",
            "Epoch: 800, test_loss = [[0.00236447]], test_acc = 20\n",
            "Epoch: 850, train_loss = [[0.00122185]], train_acc = 58\n",
            "Epoch: 850, test_loss = [[0.00201601]], test_acc = 20\n",
            "Epoch: 900, train_loss = [[0.00105794]], train_acc = 58\n",
            "Epoch: 900, test_loss = [[0.0017572]], test_acc = 20\n",
            "Epoch: 950, train_loss = [[0.00093327]], train_acc = 58\n",
            "Epoch: 950, test_loss = [[0.00155989]], test_acc = 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMeCQr_iapu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}