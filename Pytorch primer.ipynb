{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9d67f2-93bc-46a9-8dbc-ed50a2b2f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision.io import read_image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a1a7c-5b7f-43cc-90f0-4b67ffa7dcb7",
   "metadata": {},
   "source": [
    "- Basics - backward, requires_grad_, Multivariate, reshape, view, min, max, argmax, argmin, class, cat, stack, squeeze, unsqueeze, np.expand_dims\n",
    "- medium - changing dtype of tensors, handle boolean, any, all, detach, to numpy, from numpy, dim = -1, splitting train test, dataset, transform, transform compose, transform normalize with training data, convolve (numpy, torch)\n",
    "- Advanced - relu, leakyrelu, variable lr - cosine, batchnorm, dropout, CNN, RNN, LSTM, customdataset, number of trainable weights, freezing some layers in CNN\n",
    "\n",
    "\n",
    "- ==`main`\n",
    "- `Pandas` - sep, header ignore, datetime, index set, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e4875-daea-4798-9ace-acc12ca89cb7",
   "metadata": {},
   "source": [
    "### Basic Torch - Numpy primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a81e6d83-4e89-42e1-b89d-993f45a3051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 4, 5])\n",
      "tensor([[1, 2, 4, 5],\n",
      "        [2, 4, 5, 6]])\n",
      "[[1 2 4 5]\n",
      " [2 4 5 6]]\n",
      "tensor([[1, 2, 4, 5],\n",
      "        [2, 4, 5, 6]], dtype=torch.int32)\n",
      "[[0.861423   0.64127576 0.5306234 ]\n",
      " [0.40121812 0.538812   0.4806022 ]]\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "tensor([[0.6963, 0.6340, 0.4430, 0.3859],\n",
      "        [0.0474, 0.5547, 0.1294, 0.8793]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.1546, 0.0309, 0.1632],\n",
      "        [0.1995, 0.7144, 0.1431]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.Size([6, 4]) torch.Size([3, 8])\n",
      "torch.Size([2, 3, 4]) torch.Size([3, 2, 4]) torch.Size([3, 4, 2]) torch.Size([3, 4, 2])\n",
      "\n",
      "----------------------- Arithmetic Operations -----------------------\n",
      "torch.Size([3, 3])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 4])\n",
      "tensor(6.0451)\n",
      "tensor([1.7931, 0.3510, 1.7563, 2.1447])\n",
      "tensor([1.5683, 1.8832, 2.5936])\n",
      "torch.Size([3, 4]) torch.Size([3, 4])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# tensor from list\n",
    "a = [1,2,4,5]\n",
    "print(torch.tensor(a))\n",
    "\n",
    "a2 = [[1,2,4,5],[2,4,5,6]]\n",
    "print(torch.tensor(a2))\n",
    "tns_a2 = torch.tensor(a2)\n",
    "\n",
    "# tensor from numpy array\n",
    "a3 = np.array(a2)\n",
    "print(a3)\n",
    "print(torch.from_numpy(a3))\n",
    "\n",
    "## numpy from tensor\n",
    "print(torch.rand(2,3).numpy())\n",
    "\n",
    "# tensor from another tensor\n",
    "print(torch.ones_like(tns_a2))\n",
    "print(torch.rand_like(tns_a2, dtype = torch.float))\n",
    "\n",
    "# tensor from known shapes\n",
    "shape = (2,3)\n",
    "print(torch.zeros(shape))\n",
    "print(torch.ones(shape))\n",
    "print(torch.rand(shape))\n",
    "\n",
    "print(torch.ones(3,3), '\\n', torch.ones((3,3)))\n",
    "\n",
    "# joining tensors\n",
    "tensor1 = torch.rand(3,4)\n",
    "tensor2 = torch.rand(3,4)\n",
    "## cat\n",
    "cat_0 = torch.cat([tensor1, tensor2], dim=0)\n",
    "cat_1 = torch.cat([tensor1, tensor2], dim=1)\n",
    "print(cat_0.shape, cat_1.shape)\n",
    "\n",
    "## stack\n",
    "stack_0 = torch.stack([tensor1, tensor2],dim=0)\n",
    "stack_1 = torch.stack([tensor1, tensor2],dim=1)\n",
    "stack_2 = torch.stack([tensor1, tensor2],dim=2)\n",
    "stack__1 = torch.stack([tensor1, tensor2],dim=-1)\n",
    "print(stack_0.shape, stack_1.shape, stack_2.shape, stack__1.shape)\n",
    "\n",
    "print('\\n----------------------- Arithmetic Operations -----------------------')\n",
    "a = torch.rand(3,4)\n",
    "b = a.T\n",
    "\n",
    "# matrix multiplication\n",
    "c = a.matmul(b); print(c.shape)\n",
    "d = b.matmul(a); print(d.shape)\n",
    "## or\n",
    "e = torch.matmul(a,b); print(e.shape)\n",
    "\n",
    "# element-wise multiplication\n",
    "f = a * a; print(f.shape)\n",
    "\n",
    "# sum along axis\n",
    "print(a.sum())\n",
    "print(a.sum(dim=0))\n",
    "print(a.sum(dim=1))\n",
    "\n",
    "# squeeze\n",
    "a = torch.randn(3,4)\n",
    "b = a.squeeze() # remove axis that has 1 in it\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "a = torch.randn(64,1,28,28)\n",
    "b = a.squeeze() # remove axis that has 1 in it\n",
    "print(a.shape, b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c11cb-8fb3-4c27-856b-5da0945f9abb",
   "metadata": {},
   "source": [
    "#### Primer on Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d53ab9e-b075-4a6a-8dbd-45a314df4ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  tensor([[ 0.1376,  0.5324,  0.3588],\n",
      "        [-0.2198, -1.8693, -0.2780],\n",
      "        [-0.5774,  0.1756, -1.3968],\n",
      "        [ 0.3604,  0.7431, -0.4732],\n",
      "        [-0.9053, -0.7781, -0.0913]], requires_grad=True)\n",
      "b:  tensor([-1.6597, -0.1103,  0.5868], requires_grad=True)\n",
      "w.grad:  None\n",
      "b.grad:  None\n",
      "loss:  tensor(3.8616, grad_fn=<MseLossBackward0>)\n",
      "w:  tensor([[ 0.1376,  0.5324,  0.3588],\n",
      "        [-0.2198, -1.8693, -0.2780],\n",
      "        [-0.5774,  0.1756, -1.3968],\n",
      "        [ 0.3604,  0.7431, -0.4732],\n",
      "        [-0.9053, -0.7781, -0.0913]], requires_grad=True)\n",
      "b:  tensor([-1.6597, -0.1103,  0.5868], requires_grad=True)\n",
      "w.grad:  None\n",
      "b.grad:  None\n",
      "w.grad:  tensor([[-1.9095, -0.8712, -0.8624],\n",
      "        [-1.9095, -0.8712, -0.8624],\n",
      "        [-1.9095, -0.8712, -0.8624],\n",
      "        [-1.9095, -0.8712, -0.8624],\n",
      "        [-1.9095, -0.8712, -0.8624]])\n",
      "b.grad:  tensor([-1.9095, -0.8712, -0.8624])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(5, 3, requires_grad = True)\n",
    "b = torch.randn(3, requires_grad = True)\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "print('w.grad: ', w.grad)\n",
    "print('b.grad: ', b.grad)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(z, y)\n",
    "print('loss: ', loss)\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "print('w.grad: ', w.grad)\n",
    "print('b.grad: ', b.grad)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('w.grad: ', w.grad)\n",
    "print('b.grad: ', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce2bbc4-5aee-4c97-b699-aaca2381aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.])\n",
      "tensor([32.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([4.])\n",
    "x.requires_grad_()\n",
    "y  = 4*x**2 + 3\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# or\n",
    "\n",
    "x = torch.tensor([4.], requires_grad = True)\n",
    "y  = 4*x**2 + 3\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "# how can you freeze some selected layers of a model in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01e359-014c-4bca-8c68-2cdb38baf6ab",
   "metadata": {},
   "source": [
    "#### High level Deep Learning Primer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a6eed-588b-42da-b3ff-c958aa0f90cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train ds = Dataset(......, transform = ToTensor())  # list of tuples(x,y)\n",
    "test_ds = Dataset(......, transform = ToTensor())\n",
    "\n",
    "bs = 64\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size= bs)\n",
    "test_loader = DataLoader(test_ds, batch_size= bs)\n",
    "\n",
    "for X,y in test_loader:\n",
    "    print(X.shape, y.shape)\n",
    "    break\n",
    "## or\n",
    "X, y = next(iter(train_loader))\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # GT as class indices # Pred as logits\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for b, (X,y) in enumerate(dataloader):\n",
    "        X, y =  X.to(device), y.to(device)\n",
    "        # get predictions\n",
    "        pred = model(X)\n",
    "        \n",
    "        # compute loss and accuracy\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # getting final metrics for epoch\n",
    "    train_loss /= size\n",
    "    correct /= size\n",
    "    print(f'train loss: {train_loss}, train accuracy: {correct*100}')\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, corr = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(test_loss, correct*100)\n",
    "\n",
    "## We didn't return anything from training loop and test loops\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch+1}\\n----------------------------------------')\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "\n",
    "# Save and Load model - Method-1\n",
    "## save model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print('Model saved!')\n",
    "\n",
    "## load model \n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Save and Load model - Method-2 (In this we don't need to instantiate the model while loading, unlike method-1)\n",
    "## save model\n",
    "torch.save(model, 'model.pth')\n",
    "## load model\n",
    "model = torch.load('model.pth')\n",
    "\n",
    "\n",
    "# predictions\n",
    "classes = []\n",
    "\n",
    "model.eval()\n",
    "x,y = test_ds[0][0], test_ds[0][1]\n",
    "x = x.to(device)\n",
    "pred_probab_dist = model(x)\n",
    "pred_label = pred_probab_dist.argmax(0)#.type(torch.Long) and not float because list cannot be indexed using float\n",
    "pred_label_name = classes[pred_label]\n",
    "print(f'Actual- {classes[y]}, Prediction - {pred_label_name}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860020a5-73af-40b8-8486-6a17bb10472d",
   "metadata": {},
   "source": [
    "#### writing a custom `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50e591-3ef3-4d4d-a23e-e68c17709988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level overview\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, datasources, transform=None, target_transform = None)\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return image, label\n",
    "\n",
    "# sample-1: Dataset for image classification for images in a directory and label details in a .csv file\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform = None, target_transform = None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        im_pth = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = read_image(im_pth)\n",
    "        label = self.img_labels.iloc[idx,1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.transform(label)\n",
    "        return image, label\n",
    "\n",
    "# transform\n",
    "transform = ToTensor()\n",
    "target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)) # we dont need it\n",
    "# we can ignore target_transform. Dont use it. keep targets as class indices instead of one-hot encoding and use CEloss with model logits and class indices as gt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b309d33-4598-4e67-be0d-6179c7889fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC for binary classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2) ######\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "\n",
    "        \n",
    "\n",
    "# FC for multiclass classification\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10) ####\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "\n",
    "# case-1 loss is CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "for b, (X,y) in enumerate(dataloader):\n",
    "    X, y =  X.to(device), y.dtype(torch.long).to(device) # make sure y is long type and not float\n",
    "    logits = model(X)\n",
    "    probabs = nn.Softmax(dim = 1)(logits)\n",
    "    pred_label = probabs.argmax(1)\n",
    "\n",
    "# ANN for Regression 1 independent variable\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1) #### for predicting 1 value\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "\n",
    "\n",
    "# FC for Regression 2 or more independent variable\n",
    "        # - There are tons of ways to do it. \n",
    "        # - 1. Having n neurons in output layer\n",
    "        # - 2. completely independent n separate NNs\n",
    "        # - 3. common base and n separate heads (last few layers separate)\n",
    "        \n",
    "\n",
    "# CNN for binary classification\n",
    "\n",
    "# CNN for multiclass classification\n",
    "\n",
    "\n",
    "\n",
    "# RNN for binary classification\n",
    "\n",
    "# RNN for multiclass classification\n",
    "\n",
    "# RNN for Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f52d9-ca52-4780-a921-6c0088bb87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prebuilt-CNN for binary classification\n",
    "\n",
    "# Prebuilt-CNN for multiclass classification\n",
    "\n",
    "# class imbalance\n",
    "\n",
    "# lr scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b90e09-43c1-4552-aafc-4b567df0d008",
   "metadata": {},
   "source": [
    "#### working of loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3908a58-3055-4eb5-b22f-041709af1050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 9, 9, 8, 9, 4, 2, 8, 0, 0, 8, 5, 3, 6, 9, 3, 1, 4, 3, 3, 5, 6, 4, 8,\n",
      "        4, 4, 8, 5, 3, 0, 9, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4427)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. CrossEntropyLoss -  GT as class indices # Pred as logits\n",
    "loss = nn.CrossEntropyLoss()\n",
    "logits = torch.randn(32,10)\n",
    "gt = torch.empty(32, dtype = torch.long).random_(10)\n",
    "print(gt)\n",
    "loss(logits, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b29fa52-84de-4919-b142-130e29f700e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3209, 0.0211, 0.0347, 0.1563, 0.0221, 0.1124, 0.1148, 0.2059, 0.0030,\n",
       "         0.0089],\n",
       "        [0.0292, 0.2299, 0.0497, 0.1478, 0.0716, 0.1460, 0.1361, 0.0801, 0.0152,\n",
       "         0.0945],\n",
       "        [0.2013, 0.0754, 0.2527, 0.0430, 0.0993, 0.1244, 0.0180, 0.1191, 0.0445,\n",
       "         0.0225],\n",
       "        [0.2758, 0.2679, 0.0712, 0.0696, 0.0071, 0.1234, 0.0409, 0.0088, 0.0497,\n",
       "         0.0856],\n",
       "        [0.0119, 0.0872, 0.0566, 0.0623, 0.0607, 0.2982, 0.0809, 0.0963, 0.1792,\n",
       "         0.0666],\n",
       "        [0.0626, 0.0527, 0.0171, 0.0458, 0.1325, 0.0415, 0.0316, 0.0607, 0.0787,\n",
       "         0.4768],\n",
       "        [0.0382, 0.0817, 0.1429, 0.0442, 0.0474, 0.3767, 0.0660, 0.1314, 0.0300,\n",
       "         0.0415],\n",
       "        [0.0846, 0.2117, 0.0170, 0.0404, 0.0749, 0.0110, 0.1018, 0.0585, 0.3610,\n",
       "         0.0391],\n",
       "        [0.2916, 0.2613, 0.1002, 0.0517, 0.0256, 0.0706, 0.0392, 0.0490, 0.0756,\n",
       "         0.0351],\n",
       "        [0.1848, 0.1889, 0.1394, 0.0589, 0.0787, 0.0136, 0.0249, 0.2205, 0.0436,\n",
       "         0.0467],\n",
       "        [0.0492, 0.3393, 0.0390, 0.0889, 0.1560, 0.0616, 0.0373, 0.0496, 0.1482,\n",
       "         0.0309],\n",
       "        [0.0538, 0.0560, 0.0934, 0.1019, 0.1379, 0.1938, 0.0764, 0.0462, 0.2248,\n",
       "         0.0157],\n",
       "        [0.1848, 0.1486, 0.0332, 0.1040, 0.0292, 0.0313, 0.0524, 0.2003, 0.0927,\n",
       "         0.1235],\n",
       "        [0.0182, 0.0295, 0.0361, 0.1440, 0.0680, 0.0836, 0.0305, 0.2856, 0.1287,\n",
       "         0.1758],\n",
       "        [0.0961, 0.0068, 0.2374, 0.3068, 0.0750, 0.0313, 0.0496, 0.0240, 0.1131,\n",
       "         0.0599],\n",
       "        [0.3254, 0.0103, 0.0063, 0.0278, 0.0418, 0.0301, 0.1599, 0.0355, 0.0336,\n",
       "         0.3293],\n",
       "        [0.1112, 0.0570, 0.0086, 0.1038, 0.2340, 0.0723, 0.2030, 0.0139, 0.0753,\n",
       "         0.1209],\n",
       "        [0.1316, 0.0091, 0.0440, 0.1264, 0.2119, 0.0904, 0.1996, 0.1426, 0.0165,\n",
       "         0.0279],\n",
       "        [0.0910, 0.3092, 0.0479, 0.0635, 0.0824, 0.1025, 0.0338, 0.0471, 0.0824,\n",
       "         0.1401],\n",
       "        [0.2574, 0.0207, 0.0148, 0.0341, 0.0486, 0.0499, 0.0402, 0.1498, 0.0724,\n",
       "         0.3121],\n",
       "        [0.0842, 0.1154, 0.0238, 0.0904, 0.0278, 0.0469, 0.1080, 0.1870, 0.1851,\n",
       "         0.1315],\n",
       "        [0.0382, 0.0124, 0.0843, 0.2262, 0.1945, 0.2231, 0.0771, 0.0739, 0.0061,\n",
       "         0.0643],\n",
       "        [0.3369, 0.2050, 0.0263, 0.0327, 0.0831, 0.0312, 0.0083, 0.0495, 0.1132,\n",
       "         0.1139],\n",
       "        [0.1096, 0.1826, 0.0167, 0.0787, 0.0222, 0.1730, 0.0536, 0.0476, 0.0855,\n",
       "         0.2304],\n",
       "        [0.0782, 0.0325, 0.0996, 0.1441, 0.0633, 0.0268, 0.3036, 0.0262, 0.1504,\n",
       "         0.0751],\n",
       "        [0.0970, 0.0901, 0.1020, 0.0647, 0.2634, 0.0552, 0.0692, 0.1372, 0.0525,\n",
       "         0.0687],\n",
       "        [0.0558, 0.0505, 0.0171, 0.0187, 0.4179, 0.1140, 0.0537, 0.0408, 0.1780,\n",
       "         0.0535],\n",
       "        [0.0175, 0.1166, 0.1838, 0.0550, 0.0033, 0.2668, 0.0288, 0.0359, 0.2885,\n",
       "         0.0038],\n",
       "        [0.0579, 0.1096, 0.0760, 0.0525, 0.3957, 0.0538, 0.0428, 0.1314, 0.0434,\n",
       "         0.0368],\n",
       "        [0.0719, 0.0126, 0.1182, 0.1444, 0.0432, 0.0203, 0.0509, 0.0558, 0.1866,\n",
       "         0.2962],\n",
       "        [0.1124, 0.0212, 0.0310, 0.1222, 0.4114, 0.1034, 0.0237, 0.0872, 0.0680,\n",
       "         0.0196],\n",
       "        [0.0163, 0.0509, 0.0120, 0.0172, 0.1626, 0.2245, 0.0045, 0.3314, 0.0454,\n",
       "         0.1354]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim = 1)(logits) # probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91e25704-08ce-4e7c-b30b-6bae6e0fce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total params:  418369\n",
      "Number of trainable parameters: 16449\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1) #### for predicting 1 value\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logit = self.linear_relu_stack(x)\n",
    "        return logit\n",
    "\n",
    "## how to freeze weights of certain layers?\n",
    "model = NeuralNet()\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('total params: ', num_trainable_params)\n",
    "\n",
    "# Freeze the first linear layer\n",
    "for param in model.linear_relu_stack[0].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Find the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {num_trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba9fd73-bb59-4625-be3e-b798f2be2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401408\n",
      "512\n",
      "16384\n",
      "32\n",
      "32\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16449"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=0\n",
    "for p in model.parameters():\n",
    "    print(p.flatten().shape[0])\n",
    "    if p.requires_grad:\n",
    "        count+=p.flatten().shape[0]\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b19f31-ef77-44be-b53e-fc166e9d39e1",
   "metadata": {},
   "source": [
    "##### using LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedbcee-400f-4c1e-af26-d041ebd472a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, loss function, and optimizer as before\n",
    "model = NeuralNet()\n",
    "loss_fn = nn.CrossEntropyLoss() # GT as class indices, Pred as logits\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) # Adjust step_size and gamma as needed\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, scheduler):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for b, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute loss and accuracy\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Getting final metrics for epoch\n",
    "    train_loss /= size\n",
    "    correct /= size\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {correct * 100}%')\n",
    "\n",
    "# Example usage:\n",
    "# Assume `train_dataloader` is your DataLoader for the training dataset\n",
    "for epoch in range(num_epochs):\n",
    "    train(train_dataloader, model, loss_fn, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16192021-9a00-4431-80b5-4c2a3c6730c7",
   "metadata": {},
   "source": [
    "#### custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81bed221-aba2-49f5-a046-1e6e8a5936e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  tensor([[ 0.3408],\n",
      "        [-2.3544],\n",
      "        [-0.3219]]) \n",
      "output tensor([[-0.1327],\n",
      "        [-0.5044],\n",
      "        [-0.7863]], grad_fn=<AddmmBackward0>)\n",
      "loss: tensor(1.2875, grad_fn=<MeanBackward0>)\n",
      "tensor([[ 1.3618, -1.1399]])\n"
     ]
    }
   ],
   "source": [
    "def my_mse_loss1(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "model = nn.Linear(2,1)\n",
    "x = torch.randn(3,2)\n",
    "target = torch.randn(3,1)\n",
    "output = model(x)\n",
    "\n",
    "print('target: ', target,'\\noutput', output)\n",
    "\n",
    "loss = my_mse_loss1(output, target)\n",
    "print('loss:', loss)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0eca51a-734a-4614-b5ce-efd361a37cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_mse_loss2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, output, target):\n",
    "        self.output = output\n",
    "        self.target = target\n",
    "        loss = torch.mean((self.output - self.target)**2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "621c187d-2150-4bb1-8905-ed7547b61bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  tensor([[-0.6287],\n",
      "        [-0.6630],\n",
      "        [ 1.3269]]) \n",
      "output tensor([[-0.0800],\n",
      "        [ 0.2978],\n",
      "        [ 0.3465]], grad_fn=<AddmmBackward0>)\n",
      "loss: tensor(0.7285, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.3061, 0.5611]])\n"
     ]
    }
   ],
   "source": [
    "def my_mse_loss1(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "model = nn.Linear(2,1)\n",
    "x = torch.randn(3,2)\n",
    "target = torch.randn(3,1)\n",
    "output = model(x)\n",
    "\n",
    "print('target: ', target,'\\noutput', output)\n",
    "\n",
    "loss_fn = my_mse_loss2()\n",
    "loss = loss_fn(output, target)\n",
    "print('loss:', loss)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ade5d6-d1ff-47b7-bffb-304318a0571b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
