{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c14cd3-89f3-4069-9f27-612d81065b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "368187ae-06ec-4c6f-a543-72d807483404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(texts):\n",
    "    vocab = []\n",
    "    for text in texts:\n",
    "        words = text.split(' ')\n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "def text_to_vector(text, vocab, one_hot = False):\n",
    "    text_vector = np.zeros((1,len(vocab)))\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        idx = word_to_idx[word]\n",
    "        if one_hot:\n",
    "            text_vector[0,idx] = 1\n",
    "        else:\n",
    "            text_vector[0,idx] += 1\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc7922d3-a9e4-4b38-b51b-117f87ef0314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in the Dictionary or Vocabulary:  15\n",
      "X.shape (7, 15)\n",
      "y.shape (7,)\n",
      "Prior_0: 0.5714285714285714, Prior_1: 0.42857142857142855\n",
      "1 ---> 1.0\n",
      "1 ---> 1.0\n",
      "1 ---> 0.727\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "spam = ['million dollar offer','secret secret secret secret offer today','secret is secret']\n",
    "not_spam = ['low price for valued customer','play secret sports today','sports is healthy','low price pizza']\n",
    "all_messages = spam + not_spam\n",
    "\n",
    "# # PREPROCESS- VOCAB\n",
    "vocab = get_vocab(all_messages)\n",
    "print('Total number of unique words in the Dictionary or Vocabulary: ', len(vocab))\n",
    "\n",
    "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_to_word = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "# # MAKE X AND y\n",
    "X = [text_to_vector(text, vocab, one_hot=True) for text in all_messages]\n",
    "X = np.concatenate(X, axis = 0); print('X.shape', X.shape)\n",
    "y = [1,1,1,0,0,0,0]\n",
    "y = np.array(y); print('y.shape',y.shape)\n",
    "\n",
    "Prior_0 = (y==0).sum()/len(y)\n",
    "Prior_1 = (y==1).sum()/len(y)\n",
    "print(f\"Prior_0: {Prior_0}, Prior_1: {Prior_1}\")\n",
    "\n",
    "texts_0 = X[np.where(y==0)[0]] # nonspam emails\n",
    "texts_1 = X[np.where(y==1)[0]] # spam emails\n",
    "\n",
    "words_count_0 = np.sum(texts_0, axis=0)\n",
    "words_count_1 = np.sum(texts_1, axis=0)\n",
    "\n",
    "words_prob_0 = words_count_0/ texts_0.shape[0]\n",
    "words_prob_1 = words_count_1/ texts_1.shape[0]\n",
    "\n",
    "for x,gt in zip(X,y):\n",
    "    idxs = np.where(x>=1)[0]\n",
    "    pi_prob_0 = np.prod(words_prob_0[idxs])\n",
    "    pi_prob_1 = np.prod(words_prob_1[idxs])\n",
    "    spam_probability = pi_prob_1*Prior_1/(pi_prob_1*Prior_1 + pi_prob_0*Prior_0)\n",
    "    print(gt, '--->', round(spam_probability,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14a2c5-5e4f-414c-873f-cf564a0d51ca",
   "metadata": {},
   "source": [
    "### Taking account the __count__ of the words in the email\n",
    "- without smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45904478-3abe-41ab-932e-bb7f9c98a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (7, 15)\n",
      "y.shape (7,)\n",
      "Prior_0: 0.5714285714285714, Prior_1: 0.42857142857142855\n",
      "words_prob_0\n",
      " [0.   0.   0.   0.25 0.25 0.25 0.5  0.5  0.25 0.25 0.25 0.25 0.5  0.25\n",
      " 0.25]\n",
      "words_prob_1\n",
      " [0.33333333 0.33333333 0.66666667 2.         0.33333333 0.33333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "1 ---> 1.0\n",
      "1 ---> 1.0\n",
      "1 ---> 0.889\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n",
      "0 ---> 0.0\n"
     ]
    }
   ],
   "source": [
    "# # MAKE X AND y\n",
    "X = [text_to_vector(text, vocab, one_hot=False) for text in all_messages] # get counts\n",
    "X = np.concatenate(X, axis = 0); print('X.shape', X.shape)\n",
    "y = [1,1,1,0,0,0,0]\n",
    "y = np.array(y); print('y.shape',y.shape)\n",
    "\n",
    "Prior_0 = (y==0).sum()/len(y)\n",
    "Prior_1 = (y==1).sum()/len(y)\n",
    "print(f\"Prior_0: {Prior_0}, Prior_1: {Prior_1}\")\n",
    "\n",
    "texts_0 = X[np.where(y==0)[0]] # nonspam emails\n",
    "texts_1 = X[np.where(y==1)[0]] # spam emails\n",
    "\n",
    "words_count_0 = np.sum(texts_0, axis=0)\n",
    "words_count_1 = np.sum(texts_1, axis=0)\n",
    "\n",
    "# words_prob_0 = texts_0/words_count_0 # fail due to non-smoothing\n",
    "\n",
    "words_prob_0 = words_count_0/ texts_0.shape[0]\n",
    "words_prob_1 = words_count_1/ texts_1.shape[0]\n",
    "\n",
    "print('words_prob_0\\n', words_prob_0)\n",
    "print('words_prob_1\\n', words_prob_1)\n",
    "\n",
    "for x,gt in zip(X,y):\n",
    "    idxs = np.where(x>=1)[0]\n",
    "    pi_prob_0 = np.prod(words_prob_0[idxs])\n",
    "    pi_prob_1 = np.prod(words_prob_1[idxs])\n",
    "    spam_probability = pi_prob_1*Prior_1/(pi_prob_1*Prior_1 + pi_prob_0*Prior_0)\n",
    "    print(gt, '--->', round(spam_probability,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d777e847-962d-43b3-9bb4-2b394a6d3b35",
   "metadata": {},
   "source": [
    "__Downside of counting the frequency of words without smoothing is that the probabilty can become greater than 1 -> which makes no sense__\n",
    "\n",
    "__Another downside of counting the frequency of words without smoothing is that for unseen words probab=0 -> spam_probab = 0/0 -> not good__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97681300-f065-4060-9005-e01cf2eb0410",
   "metadata": {},
   "source": [
    "### Taking account the __count__ of the words in the email\n",
    "- with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1484ac9-6bbc-4ff5-ad9c-8f190dabfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fde7edd-651d-456b-ab4f-2473107b17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (7, 15)\n",
      "y.shape (7,)\n",
      "Prior_0: 0.5714285714285714, Prior_1: 0.42857142857142855\n",
      "words_prob_0\n",
      " [0.05263158 0.05263158 0.05263158 0.10526316 0.10526316 0.10526316\n",
      " 0.15789474 0.15789474 0.10526316 0.10526316 0.10526316 0.10526316\n",
      " 0.15789474 0.10526316 0.10526316]\n",
      "words_prob_1\n",
      " [0.11111111 0.11111111 0.16666667 0.38888889 0.11111111 0.11111111\n",
      " 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556]\n",
      "1 ---> 0.9137\n",
      "1 ---> 0.9026\n",
      "1 ---> 0.7452\n",
      "0 ---> 0.0135\n",
      "0 ---> 0.352\n",
      "0 ---> 0.1282\n",
      "0 ---> 0.0467\n"
     ]
    }
   ],
   "source": [
    "# # MAKE X AND y\n",
    "X = [text_to_vector(text, vocab, one_hot=False) for text in all_messages] # get counts\n",
    "X = np.concatenate(X, axis = 0); print('X.shape', X.shape)\n",
    "y = [1,1,1,0,0,0,0]\n",
    "y = np.array(y); print('y.shape',y.shape)\n",
    "\n",
    "Prior_0 = (y==0).sum()/len(y)\n",
    "Prior_1 = (y==1).sum()/len(y)\n",
    "print(f\"Prior_0: {Prior_0}, Prior_1: {Prior_1}\")\n",
    "\n",
    "texts_0 = X[np.where(y==0)[0]] # nonspam emails\n",
    "texts_1 = X[np.where(y==1)[0]] # spam emails\n",
    "\n",
    "words_count_0 = np.sum(texts_0, axis=0)\n",
    "words_count_1 = np.sum(texts_1, axis=0)\n",
    "\n",
    "# words_prob_0 = texts_0/words_count_0 # fail due to non-smoothing\n",
    "\n",
    "words_prob_0 = (words_count_0 + alpha)/ (texts_0.shape[0] + alpha*len(vocab)) # also smoothing here\n",
    "words_prob_1 = (words_count_1 + alpha)/ (texts_1.shape[0] + alpha*len(vocab)) # also smoothing here\n",
    "\n",
    "print('words_prob_0\\n', words_prob_0)\n",
    "print('words_prob_1\\n', words_prob_1)\n",
    "\n",
    "for x,gt in zip(X,y):\n",
    "    idxs = np.where(x>=1)[0]\n",
    "    pi_prob_0 = np.prod(words_prob_0[idxs])\n",
    "    pi_prob_1 = np.prod(words_prob_1[idxs])\n",
    "    spam_probability = pi_prob_1*Prior_1/(pi_prob_1*Prior_1 + pi_prob_0*Prior_0)\n",
    "    print(gt, '--->', round(spam_probability,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95de6f-bab7-4b17-859d-61aa60222876",
   "metadata": {},
   "source": [
    "##### Above version still does not handle case when __new words__ are encountered which are not present in dictionary__\n",
    "\n",
    "To handle this, there are 2 ways:\n",
    "1. __Ignore it__. Implement a check in `text_to_vector` to verify word exists in vocab only then process else ignore\n",
    "2. __Add `unknown` token in vocab__. whenever word is not in vocab -> count it as `unkown` -> practically, keep record of which words as classified as `unkown` and later add them in the vocabulary in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817cfa2-4372-4e4c-8868-6f27b09573f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
