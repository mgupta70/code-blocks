{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JzCugtH4Ntnq"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doE_RM5txUpg"
   },
   "source": [
    "<img src=\"imgs/neuron1.jpg\" alt=\"Neuron1\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4CTuNqwN20o",
    "outputId": "b29a1721-2233-4fcb-a557-1946494a3538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "  return np.mean((y_pred-y_true)**2)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias, activation = 'sigmoid'):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    self.activation = activation\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    #self.inputs = self.inputs\n",
    "    z = np.dot(self.weights, inputs) + self.bias\n",
    "    if self.activation=='sigmoid':\n",
    "      h = sigmoid(z)\n",
    "    else:\n",
    "      h = z\n",
    "    return h\n",
    "\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dojxSq2Q4Rkw"
   },
   "source": [
    "### Simple feedforward Neural Network for Regression\n",
    "  - no activation at output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3DgBtG5tZ_D"
   },
   "source": [
    "<img src=\"imgs/neuron2.jpg\" alt=\"Neuron2\" style=\"width:400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BGXHgyZLO6Dl"
   },
   "outputs": [],
   "source": [
    "class ThreeNeuronsFeedForwardNN:\n",
    "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    self.n1 = Neuron(weights1, bias1)\n",
    "    self.n2 = Neuron(weights2, bias2)\n",
    "    self.n3 = Neuron(weights3, bias3, activation='none')\n",
    "\n",
    "  def feedforwardNN(self, inputs):\n",
    "    h1 = self.n1.feedforward(inputs)\n",
    "    h2 = self.n2.feedforward(inputs)\n",
    "\n",
    "    inp_o = np.array([h1,h2]) # input to output neuron\n",
    "    h3 = self.n3.feedforward(inp_o)\n",
    "    return h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2rPJNjDPDDZ",
    "outputId": "5513ac5d-a921-469c-d62d-12c31ed38b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9525741268224334\n"
     ]
    }
   ],
   "source": [
    "weights1 = np.array([0, 1]); bias1 = 0\n",
    "weights2 = np.array([0, 1]); bias2 = 0\n",
    "weights3 = np.array([0, 1]); bias3 = 0\n",
    "\n",
    "model = ThreeNeuronsFeedForwardNN( weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "\n",
    "x = np.array([2, 3])\n",
    "print(model.feedforwardNN(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "v2vvML71SInR"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#### Developing Neural Network from scratch for Regression #######\n",
    "##################################################################\n",
    "\n",
    "class ThreeNeuronsNNRegression:\n",
    "\n",
    "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3, print_loss=True):\n",
    "    self.print_loss = print_loss\n",
    "    self.weights1 = weights1\n",
    "    self.weights2 = weights2\n",
    "    self.weights3 = weights3\n",
    "    self.bias1 = bias1\n",
    "    self.bias2 = bias2\n",
    "    self.bias3 = bias3\n",
    "    self.n1 = Neuron(self.weights1, self.bias1)\n",
    "    self.n2 = Neuron(self.weights2, self.bias2)\n",
    "    self.n3 = Neuron(self.weights3, self.bias3, activation='none')\n",
    "\n",
    "\n",
    "  def ForwardAndBackward(self, X,Y, epochs=10, lr=0.001):\n",
    "    for epoch in range(epochs):\n",
    "      loss=0\n",
    "      for x,y in zip(X,Y):\n",
    "\n",
    "      # Run feedforward part\n",
    "        self.h1 = self.n1.feedforward(x)\n",
    "        self.h2 = self.n2.feedforward(x)\n",
    "        inp_o = np.array([self.h1,self.h2]) # input to output neuron\n",
    "        self.h3 = self.n3.feedforward(inp_o)\n",
    "\n",
    "      # after feedforward, we calculate loss\n",
    "        loss += mse_loss(self.h3, y)\n",
    "\n",
    "      # TO update parameters - we need gradients\n",
    "      ## Let's calculate them now\n",
    "\n",
    "        self.del_h1w1 =  self.h1 *(1-self.h1)*x[0]\n",
    "        self.del_h1w2 =  self.h1 *(1-self.h1)*x[1]\n",
    "        self.del_h1b1 =  self.h1 *(1-self.h1)\n",
    "\n",
    "        self.del_h2w3 =  self.h2 *(1-self.h2)*x[0]\n",
    "        self.del_h2w4 =  self.h2 *(1-self.h2)*x[1]\n",
    "        self.del_h2b2 =  self.h2 *(1-self.h2)\n",
    "\n",
    "        self.del_h3w5 =  self.h1\n",
    "        self.del_h3w6 =  self.h2\n",
    "        self.del_h3b3 =  np.ones(1)\n",
    "\n",
    "        self.del_h3h1 =  self.weights3[0]\n",
    "        self.del_h3h2 =  self.weights3[1]\n",
    "\n",
    "        self.del_Lossh3 = 2*(self.h3-y)\n",
    "\n",
    "      # Now we add backpropagation part (Chain rule)\n",
    "\n",
    "        del_L1w1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w1\n",
    "        del_L1w2 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w2\n",
    "        del_L1b1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1b1\n",
    "\n",
    "        del_L1w3 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w3\n",
    "        del_L1w4 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w4\n",
    "        del_L1b2 = self.del_Lossh3 * self.del_h3h2 * self.del_h2b2\n",
    "\n",
    "        del_L1w5 = self.del_Lossh3 * self.del_h3w5\n",
    "        del_L1w6 = self.del_Lossh3 * self.del_h3w6\n",
    "        del_L1b3 = self.del_Lossh3 * self.del_h3b3\n",
    "\n",
    "      # update parameters\n",
    "        self.weights1[0] = self.weights1[0] - lr*(del_L1w1.item()) # w1\n",
    "        self.weights1[1] = self.weights1[1] - lr*(del_L1w2.item()) # w2\n",
    "        self.weights2[0] = self.weights2[0] - lr*(del_L1w3.item()) # w3\n",
    "        self.weights2[1] = self.weights2[1] - lr*(del_L1w4.item()) # w4\n",
    "        self.weights3[0] = self.weights3[0] - lr*(del_L1w5.item()) # w5\n",
    "        self.weights3[1] = self.weights3[1] - lr*(del_L1w6.item()) # w6\n",
    "        self.bias1[0] = self.bias1[0] - lr*(del_L1b1.item())       # b1\n",
    "        self.bias2[0] = self.bias2[0] - lr*(del_L1b2.item())       # b2\n",
    "        self.bias3[0] = self.bias3[0] - lr*(del_L1b3.item())       # b3\n",
    "\n",
    "      # print loss after each epoch\n",
    "      if self.print_loss and epoch%10==0:\n",
    "        print(f'Epoch: {epoch}: {x}, {y},  MSE train loss: {loss}')\n",
    "\n",
    "\n",
    "  def predict(self,x):\n",
    "    h1 = self.n1.feedforward(x)\n",
    "    h2 = self.n2.feedforward(x)\n",
    "    inp_o = np.array([h1,h2]) # input to output neuron\n",
    "    h3 = self.n3.feedforward(inp_o)\n",
    "    return h3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8JbznSaPFAV",
    "outputId": "9c55633f-bf15-4f2b-a949-f53945b1a456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (20, 2)\n",
      "Y.shape (20,)\n",
      "Epoch: 0: [1.1 2.5], 17.618832058640972,  MSE train loss: 1197.9120911577654\n",
      "Epoch: 10: [1.1 2.5], 17.618832058640972,  MSE train loss: 111.24470533785582\n",
      "Epoch: 20: [1.1 2.5], 17.618832058640972,  MSE train loss: 23.623820481313142\n",
      "Epoch: 30: [1.1 2.5], 17.618832058640972,  MSE train loss: 11.497900551449263\n",
      "Epoch: 40: [1.1 2.5], 17.618832058640972,  MSE train loss: 8.732020202105623\n",
      "Epoch: 50: [1.1 2.5], 17.618832058640972,  MSE train loss: 7.293855480509466\n",
      "Epoch: 60: [1.1 2.5], 17.618832058640972,  MSE train loss: 6.119170393205819\n",
      "Epoch: 70: [1.1 2.5], 17.618832058640972,  MSE train loss: 5.084918468373129\n",
      "Epoch: 80: [1.1 2.5], 17.618832058640972,  MSE train loss: 4.190902612893781\n",
      "Epoch: 90: [1.1 2.5], 17.618832058640972,  MSE train loss: 3.437477955510783\n",
      "\n",
      "           Actual -----> Predictions: \n",
      "3.0688320586409725 ----> 4.01580196466111\n",
      "3.834621532325184 ----> 4.396668018633475\n",
      "4.600411006009392 ----> 4.852101655977548\n",
      "5.366200479693604 ----> 5.374260577246\n",
      "6.131989953377816 ----> 5.951657798437565\n",
      "6.897779427062024 ----> 6.575181602202538\n",
      "7.6635689007462355 ----> 7.243213379205023\n",
      "8.429358374430446 ----> 7.962446934301932\n",
      "9.195147848114654 ----> 8.74364765512882\n",
      "9.960937321798866 ----> 9.594280423444609\n",
      "10.726726795483074 ----> 10.511105283646996\n",
      "11.492516269167286 ----> 11.475763244993956\n",
      "12.258305742851498 ----> 12.455468172478517\n",
      "13.02409521653571 ----> 13.409040535390591\n",
      "13.789884690219917 ----> 14.296051880348756\n",
      "14.55567416390413 ----> 15.085298954694693\n",
      "15.321463637588337 ----> 15.759473100519276\n",
      "16.087253111272553 ----> 16.315210804521683\n",
      "16.85304258495676 ----> 16.759879328182286\n",
      "17.618832058640972 ----> 17.10721375616792\n"
     ]
    }
   ],
   "source": [
    "# y = m1x1 + m2x2 + c\n",
    "X1 = np.linspace(start=0,stop=1.1, num=20)#; print('X1.shape', X1.shape)\n",
    "X2 = np.linspace(start=0,stop=2.5, num=20)#; print('X2.shape', X2.shape)\n",
    "# X1 and X2 needs to be of same size\n",
    "\n",
    "n = X1.shape[0]\n",
    "m1 = 3\n",
    "m2 = 4.5\n",
    "b1 = 4\n",
    "b2 = 6\n",
    "noise = np.random.randn()*5\n",
    "\n",
    "# x0 = np.ones_like(X1); print('x0.shape', x0.shape) # not required in neural networks because Neuron has in-built bias parameter\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X = np.stack([X1, X2], axis=1); print('X.shape', X.shape)\n",
    "Y = m1*X1 + b1 + m2*X2 + b2 + noise; print('Y.shape', Y.shape)\n",
    "\n",
    "weights1 = np.random.randn(2)\n",
    "weights2 = np.random.randn(2)\n",
    "weights3 = np.random.randn(2)\n",
    "bias1 = np.random.randn(1)\n",
    "bias2 = np.random.randn(1)\n",
    "bias3 = np.random.randn(1)\n",
    "\n",
    "# Train our neural network!\n",
    "model = ThreeNeuronsNNRegression(weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "model.ForwardAndBackward(X,Y, lr=0.01, epochs=100)\n",
    "\n",
    "print('\\n           Actual -----> Predictions: ')\n",
    "for x,y in zip(X,Y):\n",
    "  print(f'{y} ----> {model.predict(x).item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X745jEpEtIca"
   },
   "source": [
    "__Congrats: We have succesfully developed our NN from scratch using just numpy.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqxg1CWWptii"
   },
   "source": [
    "Let's also build a neural network from scratch for __classification__.\n",
    "Notice that for classification - output neuron will have a sigmoid activation. So, the gradient calculation for h3 w.r.t w5, w6, b3, h1 & h2 will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "A2ftRmcZnOtk"
   },
   "outputs": [],
   "source": [
    "def ce_loss(y_pred, y_true):\n",
    "  return -np.mean(y_true*np.log(y_pred)+ (1-y)*np.log(1-y_pred))\n",
    "\n",
    "\n",
    "##################################################################\n",
    "#### Developing Neural Network from scratch for Classification #######\n",
    "##################################################################\n",
    "\n",
    "class ThreeNeuronsNNClassification:\n",
    "\n",
    "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3, print_loss=True):\n",
    "    self.print_loss = print_loss\n",
    "    self.weights1 = weights1\n",
    "    self.weights2 = weights2\n",
    "    self.weights3 = weights3\n",
    "    self.bias1 = bias1\n",
    "    self.bias2 = bias2\n",
    "    self.bias3 = bias3\n",
    "    self.n1 = Neuron(self.weights1, self.bias1)\n",
    "    self.n2 = Neuron(self.weights2, self.bias2)\n",
    "    self.n3 = Neuron(self.weights3, self.bias3, activation='sigmoid')\n",
    "\n",
    "\n",
    "  def ForwardAndBackward(self, X,Y, epochs=10, lr=0.001):\n",
    "    for epoch in range(epochs):\n",
    "      loss=0\n",
    "      for x,y in zip(X,Y):\n",
    "\n",
    "      # Run feedforward part\n",
    "        self.h1 = self.n1.feedforward(x)\n",
    "        self.h2 = self.n2.feedforward(x)\n",
    "        inp_o = np.array([self.h1,self.h2]) # input to output neuron\n",
    "        self.h3 = self.n3.feedforward(inp_o)\n",
    "\n",
    "      # after feedforward, we calculate loss\n",
    "        loss += ce_loss(self.h3, y)\n",
    "\n",
    "      # TO update parameters - we need gradients\n",
    "      ## Let's calculate them now\n",
    "\n",
    "        self.del_h1w1 =  self.h1 *(1-self.h1)*x[0]\n",
    "        self.del_h1w2 =  self.h1 *(1-self.h1)*x[1]\n",
    "        self.del_h1b1 =  self.h1 *(1-self.h1)\n",
    "\n",
    "        self.del_h2w3 =  self.h2 *(1-self.h2)*x[0]\n",
    "        self.del_h2w4 =  self.h2 *(1-self.h2)*x[1]\n",
    "        self.del_h2b2 =  self.h2 *(1-self.h2)\n",
    "\n",
    "        self.del_h3w5 =  self.h3 *(1-self.h3)*self.h1 # updated\n",
    "        self.del_h3w6 =  self.h3 *(1-self.h3)*self.h2 # updated\n",
    "        self.del_h3b3 =  self.h3 *(1-self.h3)         # updated\n",
    "\n",
    "        self.del_h3h1 =  self.h3 *(1-self.h3)*self.weights3[0]  # updated\n",
    "        self.del_h3h2 =  self.h3 *(1-self.h3)*self.weights3[1]  # updated\n",
    "\n",
    "\n",
    "        # self.del_Lossh3 = 2*(self.h3-y) # we don't use mse loss for classification\n",
    "        self.del_Lossh3 = (self.h3 - y)/(self.h3*(1-self.h3)) # derivative of ce_loss\n",
    "\n",
    "      # Now we add backpropagation part (Chain rule)\n",
    "\n",
    "        del_L1w1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w1\n",
    "        del_L1w2 = self.del_Lossh3 * self.del_h3h1 * self.del_h1w2\n",
    "        del_L1b1 = self.del_Lossh3 * self.del_h3h1 * self.del_h1b1\n",
    "\n",
    "        del_L1w3 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w3\n",
    "        del_L1w4 = self.del_Lossh3 * self.del_h3h2 * self.del_h2w4\n",
    "        del_L1b2 = self.del_Lossh3 * self.del_h3h2 * self.del_h2b2\n",
    "\n",
    "        del_L1w5 = self.del_Lossh3 * self.del_h3w5\n",
    "        del_L1w6 = self.del_Lossh3 * self.del_h3w6\n",
    "        del_L1b3 = self.del_Lossh3 * self.del_h3b3\n",
    "\n",
    "\n",
    "      # update parameters\n",
    "        self.weights1[0] = self.weights1[0] - lr*(del_L1w1.item()) # w1\n",
    "        self.weights1[1] = self.weights1[1] - lr*(del_L1w2.item()) # w2\n",
    "        self.weights2[0] = self.weights2[0] - lr*(del_L1w3.item()) # w3\n",
    "        self.weights2[1] = self.weights2[1] - lr*(del_L1w4.item()) # w4\n",
    "        self.weights3[0] = self.weights3[0] - lr*(del_L1w5.item()) # w5\n",
    "        self.weights3[1] = self.weights3[1] - lr*(del_L1w6.item()) # w6\n",
    "        self.bias1[0] = self.bias1[0] - lr*(del_L1b1.item())       # b1\n",
    "        self.bias2[0] = self.bias2[0] - lr*(del_L1b2.item())       # b2\n",
    "        self.bias3[0] = self.bias3[0] - lr*(del_L1b3.item())       # b3\n",
    "\n",
    "      # print loss after each epoch\n",
    "      if self.print_loss and epoch%10==0:\n",
    "        print(f'Epoch: {epoch}: {x}, {y},  CE train loss: {loss}')\n",
    "\n",
    "\n",
    "  def predict(self,x):\n",
    "    h1 = self.n1.feedforward(x)\n",
    "    h2 = self.n2.feedforward(x)\n",
    "    inp_o = np.array([h1,h2]) # input to output neuron\n",
    "    h3 = self.n3.feedforward(inp_o)\n",
    "    return h3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDcaOiTpPFCt",
    "outputId": "6d55f922-d63a-469a-9769-d4ce79fdab9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (20, 2)\n",
      "Y.shape (20,)\n",
      "Epoch: 0: [1.1 2.5], 1.0,  CE train loss: 10.429837304830054\n",
      "Epoch: 10: [1.1 2.5], 1.0,  CE train loss: 5.554405237065651\n",
      "Epoch: 20: [1.1 2.5], 1.0,  CE train loss: 4.229370788285881\n",
      "Epoch: 30: [1.1 2.5], 1.0,  CE train loss: 3.139879326225674\n",
      "Epoch: 40: [1.1 2.5], 1.0,  CE train loss: 2.4533604576692385\n",
      "Epoch: 50: [1.1 2.5], 1.0,  CE train loss: 2.017299121212304\n",
      "Epoch: 60: [1.1 2.5], 1.0,  CE train loss: 1.7236988132294926\n",
      "Epoch: 70: [1.1 2.5], 1.0,  CE train loss: 1.5144718504285584\n",
      "Epoch: 80: [1.1 2.5], 1.0,  CE train loss: 1.3581824488732965\n",
      "Epoch: 90: [1.1 2.5], 1.0,  CE train loss: 1.2369266816322135\n",
      "\n",
      "Actual -----> Predictions: \n",
      "0.0 ----> 0.002825865184276334\n",
      "0.0 ----> 0.003368389367993497\n",
      "0.0 ----> 0.00432497243680419\n",
      "0.0 ----> 0.006138285892632384\n",
      "0.0 ----> 0.009907984687637199\n",
      "0.0 ----> 0.018631298217389958\n",
      "0.0 ----> 0.04097747380255652\n",
      "0.0 ----> 0.10085978056325885\n",
      "0.0 ----> 0.24470623631201874\n",
      "0.0 ----> 0.48406394309174516\n",
      "1.0 ----> 0.712041756589861\n",
      "1.0 ----> 0.8473694256401364\n",
      "1.0 ----> 0.9122638492118532\n",
      "1.0 ----> 0.9428003725653091\n",
      "1.0 ----> 0.9580255444180726\n",
      "1.0 ----> 0.9661728422965696\n",
      "1.0 ----> 0.9708074827902111\n",
      "1.0 ----> 0.9735710112014903\n",
      "1.0 ----> 0.975276807195076\n",
      "1.0 ----> 0.9763563536546673\n"
     ]
    }
   ],
   "source": [
    "X1 = np.linspace(start=0,stop=1.1, num=20) # heights\n",
    "X2 = np.linspace(start=0,stop=2.5, num=20) # weights\n",
    "X = np.stack([X1, X2], axis=1); print('X.shape', X.shape)\n",
    "Y = np.concatenate([np.array([0.]*10), np.array([1.]*10)]); print('Y.shape', Y.shape)\n",
    "\n",
    "weights1 = np.random.randn(2)\n",
    "weights2 = np.random.randn(2)\n",
    "weights3 = np.random.randn(2)\n",
    "bias1 = np.random.randn(1)\n",
    "bias2 = np.random.randn(1)\n",
    "bias3 = np.random.randn(1)\n",
    "\n",
    "# Train our neural network!\n",
    "model = ThreeNeuronsNNClassification(weights1, bias1, weights2, bias2, weights3, bias3)\n",
    "model.ForwardAndBackward(X,Y, lr=0.1, epochs=100)\n",
    "\n",
    "print('\\nActual -----> Predictions: ')\n",
    "for x,y in zip(X,Y):\n",
    "  print(f'{y} ----> {model.predict(x).item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVorwVw-sZV0"
   },
   "source": [
    "__Double Congrats: We have succesfully developed another NN from scratch using just numpy.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2hb13fUyhLrX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UN12oVOFiQ8y"
   },
   "outputs": [],
   "source": [
    "# Another helpful resource\n",
    "#### https://victorzhou.com/blog/intro-to-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O-UZCYqshcO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
