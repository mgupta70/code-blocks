{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JzCugtH4Ntnq"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    #self.inputs = self.inputs\n",
        "    z = np.dot(self.weights, inputs) + self.bias\n",
        "    activation = sigmoid(z)\n",
        "    return activation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
        "bias = 4                   # b = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4CTuNqwN20o",
        "outputId": "d022696d-be6a-424b-9018-27fa5be89563"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ThreeNeuronsFeedForwardNN:\n",
        "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3):\n",
        "    self.n1 = Neuron(weights1, bias1)\n",
        "    self.n2 = Neuron(weights2, bias2)\n",
        "    self.n3 = Neuron(weights3, bias3)\n",
        "\n",
        "  def feedforwardNN(self, inputs):\n",
        "    z1 = self.n1.feedforward(inputs)\n",
        "    z2 = self.n2.feedforward(inputs)\n",
        "\n",
        "    inp_o = np.array([z1,z2]) # input to output neuron\n",
        "    z3 = self.n3.feedforward(inp_o)\n",
        "    return z3\n",
        ""
      ],
      "metadata": {
        "id": "BGXHgyZLO6Dl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "weights1 = np.array([0, 1]); bias1 = 0\n",
        "weights2 = np.array([0, 1]); bias2 = 0\n",
        "weights3 = np.array([0, 1]); bias3 = 0\n",
        "\n",
        "model = ThreeNeuronsFeedForwardNN( weights1, bias1, weights2, bias2, weights3, bias3)\n",
        "\n",
        "x = np.array([2, 3])\n",
        "print(model.feedforwardNN(x)) # 0.7216325609518421"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2rPJNjDPDDZ",
        "outputId": "0433cf8e-2b55-4af3-fa9e-569364f9d7c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7216325609518421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_pred, y_true):\n",
        "  return np.mean((y_pred-y_true)**2)\n",
        "\n",
        "# example:\n",
        "y_true = np.array([1, 0, 0, 1])\n",
        "y_pred = np.array([0, 0, 0, 0])\n",
        "\n",
        "print(mse_loss(y_true, y_pred)) # 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jj_lxqCRkAq",
        "outputId": "e85f9cab-cbe4-43f6-ae06-d2e028e50fa9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n"
      ],
      "metadata": {
        "id": "Z6SQGSsfSGIX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####  Putting everything together\n",
        "## Useful functions\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "  return np.mean((y_pred-y_true)**2)\n",
        "\n",
        "###################################################\n",
        "#### Developing Neural Network from scratch #######\n",
        "###################################################\n",
        "\n",
        "## 1. Develop Neurons\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    #self.inputs = self.inputs\n",
        "    z = np.dot(self.weights, inputs) + self.bias\n",
        "    activation = sigmoid(z)\n",
        "    return activation\n",
        "\n",
        "## 2. Stack Neurons to create simple fully-connected 2 layer NN with 2 neuron in input layer and 1 in output layer.\n",
        "\n",
        "class ThreeNeuronsNN:\n",
        "\n",
        "  def __init__(self, weights1, bias1, weights2, bias2, weights3, bias3, print_loss=True):\n",
        "    self.print_loss = print_loss\n",
        "    self.weights1 = weights1\n",
        "    self.weights2 = weights2\n",
        "    self.weights3 = weights3\n",
        "    self.bias1 = bias1\n",
        "    self.bias2 = bias2\n",
        "    self.bias3 = bias3\n",
        "    self.n1 = Neuron(self.weights1, self.bias1)\n",
        "    self.n2 = Neuron(self.weights2, self.bias2)\n",
        "    self.n3 = Neuron(self.weights3, self.bias3)\n",
        "\n",
        "\n",
        "  def ForwardAndBackward(self, X,Y, epochs=10, lr=0.001):\n",
        "    for epoch in range(epochs):\n",
        "      loss=0\n",
        "      for x,y in zip(X,Y):\n",
        "\n",
        "      # Run feedforward part\n",
        "        self.z1 = self.n1.feedforward(x)\n",
        "        self.z2 = self.n2.feedforward(x)\n",
        "        inp_o = np.array([self.z1,self.z2]) # input to output neuron\n",
        "        self.z3 = self.n3.feedforward(inp_o)\n",
        "\n",
        "      # after feedforward, we calculate loss\n",
        "        loss += mse_loss(self.z3, y)\n",
        "\n",
        "\n",
        "      # TO update parameters - we need gradients\n",
        "      ## Let's calculate them now\n",
        "\n",
        "        self.f_z1 = sigmoid(self.z1)\n",
        "        self.f_z2 = sigmoid(self.z2)\n",
        "        self.f_z3 = sigmoid(self.z3)\n",
        "\n",
        "        self.del_Z1w1 =  self.f_z1 *(1-self.f_z1)*x[0]\n",
        "        self.del_Z1w2 =  self.f_z1 *(1-self.f_z1)*x[1]\n",
        "        self.del_Z1b1 =  self.f_z1 *(1-self.f_z1)\n",
        "\n",
        "\n",
        "        self.del_Z2w3 =  self.f_z2 *(1-self.f_z2)*x[0]\n",
        "        self.del_Z2w4 =  self.f_z2 *(1-self.f_z2)*x[1]\n",
        "        self.del_Z2b2 =  self.f_z2 *(1-self.f_z2)\n",
        "\n",
        "        self.del_Z3w5 =  self.f_z3 *(1-self.f_z3)*self.z1\n",
        "        self.del_Z3w6 =  self.f_z3 *(1-self.f_z3)*self.z2\n",
        "        self.del_Z3b3 =  self.f_z3 *(1-self.f_z3)\n",
        "\n",
        "        self.del_Z3Z1 =  self.f_z3 *(1-self.f_z3)*self.weights3[0]\n",
        "        self.del_Z3Z2 =  self.f_z3 *(1-self.f_z3)*self.weights3[1]\n",
        "\n",
        "        self.del_LossZ3 = 2*(self.z3-y)\n",
        "\n",
        "      # Now we add backpropagation part (Chain rule)\n",
        "\n",
        "        del_L1w1 = self.del_LossZ3 * self.del_Z3Z1 * self.del_Z1w1\n",
        "        del_L1w2 = self.del_LossZ3 * self.del_Z3Z1 * self.del_Z1w2\n",
        "        del_L1b1 = self.del_LossZ3 * self.del_Z3Z1 * self.del_Z1b1\n",
        "\n",
        "        del_L1w3 = self.del_LossZ3 * self.del_Z3Z2 * self.del_Z2w3\n",
        "        del_L1w4 = self.del_LossZ3 * self.del_Z3Z2 * self.del_Z2w4\n",
        "        del_L1b2 = self.del_LossZ3 * self.del_Z3Z2 * self.del_Z2b2\n",
        "\n",
        "        del_L1w5 = self.del_LossZ3 * self.del_Z3w5\n",
        "        del_L1w6 = self.del_LossZ3 * self.del_Z3w6\n",
        "        del_L1b3 = self.del_LossZ3 * self.del_Z3b3\n",
        "\n",
        "\n",
        "      # update parameters\n",
        "        self.weights1[0] = self.weights1[0] - lr*(del_L1w1.item())\n",
        "        self.weights1[1] = self.weights1[1] - lr*(del_L1w2.item())\n",
        "        self.weights2[0] = self.weights2[0] - lr*(del_L1w3.item())\n",
        "        self.weights2[1] = self.weights2[1] - lr*(del_L1w4.item())\n",
        "        self.weights3[0] = self.weights3[0] - lr*(del_L1w5.item())\n",
        "        self.weights3[1] = self.weights3[1] - lr*(del_L1w6.item())\n",
        "        self.bias1[0] = self.bias1[0] - lr*(del_L1b1.item())\n",
        "        self.bias2[0] = self.bias2[0] - lr*(del_L1b2.item())\n",
        "        self.bias3[0] = self.bias3[0] - lr*(del_L1b3.item())\n",
        "\n",
        "      # print loss after each epoch\n",
        "      if self.print_loss:\n",
        "        print(f'Epoch: {epoch}: {x}, {y},  MSE train loss: {loss}')\n",
        "\n",
        "\n",
        "  def predict(self,x):\n",
        "    z1 = self.n1.feedforward(x)\n",
        "    z2 = self.n2.feedforward(x)\n",
        "    inp_o = np.array([z1,z2]) # input to output neuron\n",
        "    z3 = self.n3.feedforward(inp_o)\n",
        "    return z3\n",
        "\n"
      ],
      "metadata": {
        "id": "v2vvML71SInR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X data containng features height and weight. (after subtracting a fixed constant height and weight)\n",
        "X = np.array([\n",
        "  [-2, -1],  # Female\n",
        "  [25, 6],   # Male\n",
        "  [17, 4],   # Male\n",
        "  [-15, -6], # Female\n",
        "])\n",
        "Y = np.array([\n",
        "  1, # F\n",
        "  0, # M\n",
        "  0, # M\n",
        "  1, # F\n",
        "])\n",
        "\n",
        "weights1 = np.random.randn(2)\n",
        "weights2 = np.random.randn(2)\n",
        "weights3 = np.random.randn(2)\n",
        "bias1 = np.random.randn(1)\n",
        "bias2 = np.random.randn(1)\n",
        "bias3 = np.random.randn(1)\n",
        "\n",
        "# Train our neural network!\n",
        "model = ThreeNeuronsNN(weights1, bias1, weights2, bias2, weights3, bias3)\n",
        "model.ForwardAndBackward(X,Y, lr=0.001, epochs=100)\n",
        "\n",
        "print('\\npredictions: ')\n",
        "for x,y in zip(X,Y):\n",
        "  pred = np.where(model.predict(x)>=0.5, 1, 0)\n",
        "  print(f'{x}  {y} ----> {model.predict(x)} --->{pred}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hb13fUyhLrX",
        "outputId": "4492dab8-26df-4956-dac0-65c27ec40f2f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0: [-15  -6], 1,  MSE train loss: 0.7400203618591396\n",
            "Epoch: 1: [-15  -6], 1,  MSE train loss: 0.7393957674151754\n",
            "Epoch: 2: [-15  -6], 1,  MSE train loss: 0.738773003013006\n",
            "Epoch: 3: [-15  -6], 1,  MSE train loss: 0.7381520606248254\n",
            "Epoch: 4: [-15  -6], 1,  MSE train loss: 0.737532932230584\n",
            "Epoch: 5: [-15  -6], 1,  MSE train loss: 0.7369156098185708\n",
            "Epoch: 6: [-15  -6], 1,  MSE train loss: 0.7363000853859826\n",
            "Epoch: 7: [-15  -6], 1,  MSE train loss: 0.7356863509394842\n",
            "Epoch: 8: [-15  -6], 1,  MSE train loss: 0.7350743984957606\n",
            "Epoch: 9: [-15  -6], 1,  MSE train loss: 0.7344642200820548\n",
            "Epoch: 10: [-15  -6], 1,  MSE train loss: 0.7338558077366996\n",
            "Epoch: 11: [-15  -6], 1,  MSE train loss: 0.7332491535096393\n",
            "Epoch: 12: [-15  -6], 1,  MSE train loss: 0.7326442494629376\n",
            "Epoch: 13: [-15  -6], 1,  MSE train loss: 0.7320410876712824\n",
            "Epoch: 14: [-15  -6], 1,  MSE train loss: 0.7314396602224751\n",
            "Epoch: 15: [-15  -6], 1,  MSE train loss: 0.7308399592179146\n",
            "Epoch: 16: [-15  -6], 1,  MSE train loss: 0.7302419767730686\n",
            "Epoch: 17: [-15  -6], 1,  MSE train loss: 0.7296457050179384\n",
            "Epoch: 18: [-15  -6], 1,  MSE train loss: 0.7290511360975138\n",
            "Epoch: 19: [-15  -6], 1,  MSE train loss: 0.7284582621722173\n",
            "Epoch: 20: [-15  -6], 1,  MSE train loss: 0.7278670754183416\n",
            "Epoch: 21: [-15  -6], 1,  MSE train loss: 0.727277568028476\n",
            "Epoch: 22: [-15  -6], 1,  MSE train loss: 0.726689732211926\n",
            "Epoch: 23: [-15  -6], 1,  MSE train loss: 0.7261035601951229\n",
            "Epoch: 24: [-15  -6], 1,  MSE train loss: 0.7255190442220244\n",
            "Epoch: 25: [-15  -6], 1,  MSE train loss: 0.7249361765545088\n",
            "Epoch: 26: [-15  -6], 1,  MSE train loss: 0.7243549494727577\n",
            "Epoch: 27: [-15  -6], 1,  MSE train loss: 0.7237753552756327\n",
            "Epoch: 28: [-15  -6], 1,  MSE train loss: 0.7231973862810427\n",
            "Epoch: 29: [-15  -6], 1,  MSE train loss: 0.7226210348263036\n",
            "Epoch: 30: [-15  -6], 1,  MSE train loss: 0.7220462932684886\n",
            "Epoch: 31: [-15  -6], 1,  MSE train loss: 0.721473153984773\n",
            "Epoch: 32: [-15  -6], 1,  MSE train loss: 0.7209016093727681\n",
            "Epoch: 33: [-15  -6], 1,  MSE train loss: 0.7203316518508501\n",
            "Epoch: 34: [-15  -6], 1,  MSE train loss: 0.7197632738584783\n",
            "Epoch: 35: [-15  -6], 1,  MSE train loss: 0.7191964678565083\n",
            "Epoch: 36: [-15  -6], 1,  MSE train loss: 0.7186312263274969\n",
            "Epoch: 37: [-15  -6], 1,  MSE train loss: 0.7180675417759979\n",
            "Epoch: 38: [-15  -6], 1,  MSE train loss: 0.7175054067288531\n",
            "Epoch: 39: [-15  -6], 1,  MSE train loss: 0.7169448137354741\n",
            "Epoch: 40: [-15  -6], 1,  MSE train loss: 0.7163857553681172\n",
            "Epoch: 41: [-15  -6], 1,  MSE train loss: 0.7158282242221531\n",
            "Epoch: 42: [-15  -6], 1,  MSE train loss: 0.7152722129163258\n",
            "Epoch: 43: [-15  -6], 1,  MSE train loss: 0.714717714093009\n",
            "Epoch: 44: [-15  -6], 1,  MSE train loss: 0.7141647204184518\n",
            "Epoch: 45: [-15  -6], 1,  MSE train loss: 0.7136132245830203\n",
            "Epoch: 46: [-15  -6], 1,  MSE train loss: 0.7130632193014308\n",
            "Epoch: 47: [-15  -6], 1,  MSE train loss: 0.7125146973129779\n",
            "Epoch: 48: [-15  -6], 1,  MSE train loss: 0.7119676513817548\n",
            "Epoch: 49: [-15  -6], 1,  MSE train loss: 0.7114220742968683\n",
            "Epoch: 50: [-15  -6], 1,  MSE train loss: 0.7108779588726464\n",
            "Epoch: 51: [-15  -6], 1,  MSE train loss: 0.7103352979488408\n",
            "Epoch: 52: [-15  -6], 1,  MSE train loss: 0.7097940843908217\n",
            "Epoch: 53: [-15  -6], 1,  MSE train loss: 0.7092543110897686\n",
            "Epoch: 54: [-15  -6], 1,  MSE train loss: 0.708715970962854\n",
            "Epoch: 55: [-15  -6], 1,  MSE train loss: 0.7081790569534198\n",
            "Epoch: 56: [-15  -6], 1,  MSE train loss: 0.7076435620311511\n",
            "Epoch: 57: [-15  -6], 1,  MSE train loss: 0.7071094791922419\n",
            "Epoch: 58: [-15  -6], 1,  MSE train loss: 0.7065768014595543\n",
            "Epoch: 59: [-15  -6], 1,  MSE train loss: 0.7060455218827761\n",
            "Epoch: 60: [-15  -6], 1,  MSE train loss: 0.7055156335385683\n",
            "Epoch: 61: [-15  -6], 1,  MSE train loss: 0.7049871295307105\n",
            "Epoch: 62: [-15  -6], 1,  MSE train loss: 0.7044600029902386\n",
            "Epoch: 63: [-15  -6], 1,  MSE train loss: 0.70393424707558\n",
            "Epoch: 64: [-15  -6], 1,  MSE train loss: 0.7034098549726803\n",
            "Epoch: 65: [-15  -6], 1,  MSE train loss: 0.7028868198951271\n",
            "Epoch: 66: [-15  -6], 1,  MSE train loss: 0.7023651350842697\n",
            "Epoch: 67: [-15  -6], 1,  MSE train loss: 0.7018447938093303\n",
            "Epoch: 68: [-15  -6], 1,  MSE train loss: 0.7013257893675134\n",
            "Epoch: 69: [-15  -6], 1,  MSE train loss: 0.7008081150841103\n",
            "Epoch: 70: [-15  -6], 1,  MSE train loss: 0.7002917643125972\n",
            "Epoch: 71: [-15  -6], 1,  MSE train loss: 0.6997767304347289\n",
            "Epoch: 72: [-15  -6], 1,  MSE train loss: 0.6992630068606301\n",
            "Epoch: 73: [-15  -6], 1,  MSE train loss: 0.6987505870288796\n",
            "Epoch: 74: [-15  -6], 1,  MSE train loss: 0.6982394644065913\n",
            "Epoch: 75: [-15  -6], 1,  MSE train loss: 0.6977296324894909\n",
            "Epoch: 76: [-15  -6], 1,  MSE train loss: 0.6972210848019882\n",
            "Epoch: 77: [-15  -6], 1,  MSE train loss: 0.6967138148972443\n",
            "Epoch: 78: [-15  -6], 1,  MSE train loss: 0.6962078163572364\n",
            "Epoch: 79: [-15  -6], 1,  MSE train loss: 0.6957030827928167\n",
            "Epoch: 80: [-15  -6], 1,  MSE train loss: 0.6951996078437689\n",
            "Epoch: 81: [-15  -6], 1,  MSE train loss: 0.6946973851788596\n",
            "Epoch: 82: [-15  -6], 1,  MSE train loss: 0.6941964084958857\n",
            "Epoch: 83: [-15  -6], 1,  MSE train loss: 0.6936966715217197\n",
            "Epoch: 84: [-15  -6], 1,  MSE train loss: 0.6931981680123486\n",
            "Epoch: 85: [-15  -6], 1,  MSE train loss: 0.6927008917529123\n",
            "Epoch: 86: [-15  -6], 1,  MSE train loss: 0.6922048365577343\n",
            "Epoch: 87: [-15  -6], 1,  MSE train loss: 0.6917099962703541\n",
            "Epoch: 88: [-15  -6], 1,  MSE train loss: 0.6912163647635505\n",
            "Epoch: 89: [-15  -6], 1,  MSE train loss: 0.6907239359393657\n",
            "Epoch: 90: [-15  -6], 1,  MSE train loss: 0.6902327037291237\n",
            "Epoch: 91: [-15  -6], 1,  MSE train loss: 0.6897426620934473\n",
            "Epoch: 92: [-15  -6], 1,  MSE train loss: 0.6892538050222692\n",
            "Epoch: 93: [-15  -6], 1,  MSE train loss: 0.6887661265348426\n",
            "Epoch: 94: [-15  -6], 1,  MSE train loss: 0.6882796206797469\n",
            "Epoch: 95: [-15  -6], 1,  MSE train loss: 0.6877942815348909\n",
            "Epoch: 96: [-15  -6], 1,  MSE train loss: 0.6873101032075136\n",
            "Epoch: 97: [-15  -6], 1,  MSE train loss: 0.6868270798341811\n",
            "Epoch: 98: [-15  -6], 1,  MSE train loss: 0.6863452055807799\n",
            "Epoch: 99: [-15  -6], 1,  MSE train loss: 0.6858644746425099\n",
            "\n",
            "predictions: \n",
            "[-2 -1]  1 ----> [0.51050756] --->[1]\n",
            "[25  6]  0 ----> [0.32221327] --->[0]\n",
            "[17  4]  0 ----> [0.32221327] --->[0]\n",
            "[-15  -6]  1 ----> [0.51199872] --->[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__We succesfully developed our NN from scratch using just numpy.__"
      ],
      "metadata": {
        "id": "X745jEpEtIca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interesting insight - We used MSE for Classification. It may or may not work. It is highly sensitive to initialization. Run the above code 10 times. Most of the times it won't converge. Sometimes it might. Why? __Refer to answer 10-d in https://github.com/mgupta70/literature/blob/main/Logistic%20Regression.pdf for more details in it__"
      ],
      "metadata": {
        "id": "xEqSVI7goaxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helpful resource\n",
        "#### https://victorzhou.com/blog/intro-to-neural-networks/"
      ],
      "metadata": {
        "id": "UN12oVOFiQ8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}